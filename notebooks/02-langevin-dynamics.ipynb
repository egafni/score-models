{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langevin Dynamics\n",
    "\n",
    "In this notebook, I'd like to explore further the use of [Langevin dynamics][langevin]\n",
    "in generating samples from an unknown distribution.\n",
    "(For now, I will stick to 1D distributions.)\n",
    "\n",
    "[langevin]: https://yang-song.github.io/blog/2021/score/#langevin-dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Yang Song's blog,\n",
    "\n",
    "> Langevin dynamics provides an MCMC procedure to sample from a distribution\n",
    "> $p(x)$ using only its score function $\\nabla_x \\log p(x)$. \n",
    "> Specifically, it initializes the chain from an arbitrary prior distribution\n",
    "> $x_0 \\sim \\pi(x)$, and then iterates the following:\n",
    ">\n",
    "> $$x_{i+1} \\leftarrow x_i + \\epsilon \\nabla_x \\log p(x) + \\sqrt{2 \\epsilon} z_i$$\n",
    "\n",
    "where $i = 0, 1, ... K$\n",
    "and $z_i \\sim Normal(0, I)$\n",
    "is a multivariate Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We're going to generate data from a mixture 1D gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random \n",
    "import jax.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ecdf(data):\n",
    "    x, y = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "key = random.PRNGKey(45)\n",
    "k1, k2, k3 = random.split(key, 3)\n",
    "\n",
    "mix1 = random.normal(k1, shape=(10000,)) * 3 - 5\n",
    "mix2 = random.normal(k2, shape=(5000,)) * 1 + 6\n",
    "\n",
    "data = np.concatenate([mix1, mix2]).flatten()\n",
    "plt.plot(*ecdf(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Gradients\n",
    "\n",
    "Great, we have a mixture Gaussian.\n",
    "Now, we need to train a score model to approximate its gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import nn_score_func, nn_model\n",
    "from score_models.losses import loss\n",
    "from functools import partial\n",
    "from jaxopt import GradientDescent\n",
    "\n",
    "\n",
    "init_fun, apply_fun = nn_model()\n",
    "k1, k2 = random.split(k3)\n",
    "_, params_init = init_fun(k1, input_shape=(None, 1))\n",
    "myloss = partial(loss, score_func=nn_score_func)\n",
    "solver = GradientDescent(fun=myloss, maxiter=1200)\n",
    "result = solver.run(params_init, batch=data.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "model_scores = vmap(partial(nn_score_func, result.params))(data).squeeze()\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from jax import lax \n",
    "\n",
    "key = random.PRNGKey(40)\n",
    "k1, k2 = random.split(key)\n",
    "x = random.normal(k2, shape=(1,)) * 3 + 0\n",
    "epsilon = 5e-3\n",
    "\n",
    "# samples = []\n",
    "# for i in range(10000):\n",
    "#     k1, k2 = random.split(k2)\n",
    "#     draw = random.normal(k1, shape=(1,))\n",
    "#     x = x + epsilon * nn_score_func(result.params, x) + np.sqrt(2 * epsilon) * draw\n",
    "#     samples.append(x)\n",
    "\n",
    "def langevin_dynamics_one_chain(\n",
    "    x: float, \n",
    "    key: random.PRNGKey, \n",
    "    n_samples: int, \n",
    "    epsilon: float, \n",
    "    score_func: Callable, \n",
    "    params,\n",
    "):\n",
    "    \"\"\"One chain of Langevin dynamics sampling for score models.\"\"\"\n",
    "\n",
    "    keys = random.split(key, n_samples)\n",
    "    def inner(prev_x, key):\n",
    "        draw = random.normal(key, shape=(1,))\n",
    "        new_x = prev_x + epsilon * score_func(params, prev_x) + np.sqrt(2 * epsilon) * draw\n",
    "        return new_x, prev_x\n",
    "\n",
    "    _, xs = lax.scan(inner, x, keys)\n",
    "    return np.concatenate(xs) \n",
    "\n",
    "\n",
    "samples = langevin_dynamics_one_chain(x, key, 10000, epsilon, nn_score_func, result.params)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, vmap across many chains\n",
    "def langevin_dynamics_many_chains(n_chains, n_samples, key, epsilon, score_func, params):\n",
    "    \"\"\"MCMC with Langevin dynamics to sample from the data generating distribution.\"\"\"\n",
    "    starter_xs = random.normal(key, shape=(n_chains,)).reshape(-1, 1)\n",
    "    keys = random.split(key, num=n_chains)\n",
    "    chain_func = partial(langevin_dynamics_one_chain, n_samples=n_samples, epsilon=epsilon, score_func=score_func, params=params)\n",
    "    samples = vmap(chain_func)(starter_xs, keys)\n",
    "    return samples\n",
    "\n",
    "chain_samples = langevin_dynamics_many_chains(n_chains=1000, n_samples=2000, key=key, epsilon=epsilon, score_func=nn_score_func, params=result.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ecdf(chain_samples.flatten()), label=\"samples\")\n",
    "plt.plot(*ecdf(data), label=\"data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMG OMG OMG! We can actually sample from the multivariate Gaussian!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some caveats:\n",
    "\n",
    "1. The weight between the two Gaussians are different, which is disturbing.\n",
    "   I am not sure whether this is because of the score function being inaccurate or not.\n",
    "   In any case, we can definitely change the score function model\n",
    "   as it is quite small at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a062a7e1adbb829192b5a56a463e7bc3d2201a3b05feadd05a7a64f9805fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
