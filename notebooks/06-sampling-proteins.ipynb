{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n",
    "np.arange(3)\n",
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Proteins\n",
    "\n",
    "In this chapter, I am going to explore \n",
    "how we might sample new protein sequences using score models.\n",
    "Score models are usually useful for sampling new _continuous_ data,\n",
    "such as audio and images,\n",
    "but I don't think we've seen much activity in the realm of sampling new _discrete_ data,\n",
    "such as text.\n",
    "This is something I have been intellectually interested in,\n",
    "given my interest in sequence machine learning since graduate school,\n",
    "where I built the seeds of what would become my Insight project,\n",
    "the [Flu Sequence Forecaster](http://fluforecaster.herokuapp.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "As we've seen before, score models start with a _density_ of input data.\n",
    "That can be 1D, such as Gaussian draws,\n",
    "or 2D, such as the half-moons data,\n",
    "or n-dimensional, such as numerical representations of proteins.\n",
    "That's where we'll start, obtaining a numerical representation of proteins\n",
    "by using a _non-variational_ autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Proteins\n",
    "\n",
    "Our autoencoder model will be a relatively simple one: basically a linear autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx \n",
    "from jax import random \n",
    "from jax.nn import sigmoid\n",
    "from jax.scipy.special import expit\n",
    "\n",
    "\n",
    "class LinearAutoEncoder(eqx.Module):\n",
    "    encoder: eqx.Module\n",
    "    decoder: eqx.Module \n",
    "\n",
    "    def __init__(self, in_size: int, latent_dim_size: int = 512, key=random.PRNGKey(45)):\n",
    "        encoder_key, decoder_key = random.split(key)\n",
    "        self.encoder = eqx.nn.Linear(in_features=in_size, out_features=latent_dim_size, key=encoder_key)\n",
    "        self.decoder = eqx.nn.Linear(in_features=latent_dim_size, out_features=in_size, key=decoder_key)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        latent = self.encode(x)\n",
    "        out = self.decoder(latent)\n",
    "        return sigmoid(out)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "lae = LinearAutoEncoder(2048)\n",
    "one_batch = random.normal(key=random.PRNGKey(42), shape=(2048,))\n",
    "\n",
    "lae(one_batch).min(), lae(one_batch).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will grab a bunch of real protein sequences to play around with.\n",
    "The FASTA file we will take is from my flu forecaster repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ericmjl/flu-sequence-predictor/master/data/20170531-H3N2-global.fasta\"\n",
    "filename = wget.download(url, out=\"/tmp/h3n2.fasta\")\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain a multiple sequence alignment\n",
    "\n",
    "One of the easiest (though not the only) ways \n",
    "to obtain numerical representations for a linear autoencoder \n",
    "is to use SeqLike to generate a multiple sequence alignment\n",
    "and then convert the alignment into a one-hot encoded NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from seqlike import aaSeqLike \n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "seqs = [aaSeqLike(s) for s in tqdm(SeqIO.parse(filename, \"fasta\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = pd.Series(seqs).sample(2000, random_state=44).seq.align()\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_oh = seqs.seq.to_onehot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jax import vmap \n",
    "import jax.numpy as np\n",
    "\n",
    "def flatten(x: np.ndarray):\n",
    "    return x.flatten()\n",
    "\n",
    "seqs_flattened = vmap(flatten)(seqs_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_hat, y):\n",
    "    xent = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "    return -np.mean(xent)\n",
    "\n",
    "binary_cross_entropy(lae(one_batch), np.round(np.clip(one_batch, 0, 1)))\n",
    "# binary_cross_entropy(one_batch, one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy(one_batch, one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(model, y, tol=1e-6):\n",
    "    \"\"\"Binary cross entropy loss function.\n",
    "    \n",
    "    :param y_hat: Batches (n >= 1) of predictions.\n",
    "    :param y: Batches (n >= 1) of ground truth data.\n",
    "    :returns: Scalar loss.\n",
    "    \"\"\"\n",
    "    y_hat = vmap(model)(y)\n",
    "    y_hat = np.clip(y_hat, tol, 1 - tol)\n",
    "    xents = binary_cross_entropy(y_hat, y)\n",
    "    return np.mean(xents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearAutoEncoder(in_size=len(seqs_flattened[0]))\n",
    "\n",
    "binary_cross_entropy_loss(model, seqs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.encode(seqs_flattened[0])\n",
    "out = model.decoder(encoded)\n",
    "sigmoid(out), model(seqs_flattened[0])\n",
    "# vmap(sigmoid)(out)\n",
    "\n",
    "\n",
    "vmap(model)(seqs_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_flattened[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "import optax \n",
    "\n",
    "\n",
    "model = LinearAutoEncoder(in_size=len(seqs_flattened[0]))\n",
    "optimizer = optax.chain(optax.adam(5e-3), optax.clip(0.001))\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "dloss = eqx.filter_value_and_grad(binary_cross_entropy_loss)\n",
    "\n",
    "n_steps = 20\n",
    "iterator = tqdm(range(n_steps))\n",
    "loss_history = []\n",
    "key = random.PRNGKey(555)\n",
    "keys = random.split(key, n_steps)\n",
    "\n",
    "for step in iterator:\n",
    "    loss_score, grads = dloss(model, seqs_flattened)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    iterator.set_description(f\"ScoreÂ· {loss_score:.3f}\")\n",
    "    loss_history.append(float(loss_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vmap(model.encoder)(seqs_flattened)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"notebook\")\n",
    "\n",
    "um = UMAP(random_state=212)\n",
    "um_embed = um.fit_transform(embeddings)\n",
    "plt.scatter(um_embed[:, 0], um_embed[:, 1])\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.title(\"UMAP Embedding of Encoded Sequences\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noising Data with Variance-Preserving SDE\n",
    "\n",
    "If you noticed, in the previous chapter, whenever we noised up data,\n",
    "our total data variance would also increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
