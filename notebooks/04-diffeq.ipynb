{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Equations\n",
    "\n",
    "We're going to take a small detour through ordinary differential equations (ODEs).\n",
    "As should become evident shortly,\n",
    "this topic is important for our journey through score modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary differential equations\n",
    "\n",
    "Before we go into probability flow ODEs, though, \n",
    "we should revisit their more, ahem, _ordinary_ counterpart,\n",
    "ordinary differential equations (ODEs).\n",
    "ODEs are usually taught in undergraduate calculus classes,\n",
    "since they involve differentiation and integration.\n",
    "I do remember encountering them in high school in Singapore,\n",
    "which is a testament to how advanced the mathematics curriculum in Singapore is.\n",
    "\n",
    "ODEs are useful models of systems\n",
    "where we believe that the rate of change of an output variable\n",
    "is a math function of some input variable.\n",
    "In abstract mathematical symbols:\n",
    "\n",
    "$$\\frac{dy}{dx} = f(x, \\theta)$$\n",
    "\n",
    "Here, $f$ simply refers to some mathematical function of $x$\n",
    "and the function's parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classic ODE example\n",
    "\n",
    "A classic ODE example that we might think of is that of a decay curve:\n",
    "\n",
    "$$\\frac{dy}{dt} = -y$$\n",
    "\n",
    "Implemented in `diffrax`, which is a JAX package for differential equations,\n",
    "and wrapped in Equinox as a parameterized function,\n",
    "we have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffrax import diffeqsolve, Tsit5, ODETerm, SaveAt\n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import equinox as eqx\n",
    "\n",
    "\n",
    "def exponential_decay_drift(t, y, args):\n",
    "    return -y\n",
    "\n",
    "\n",
    "class ODE(eqx.Module):\n",
    "    drift: callable\n",
    "\n",
    "    def __call__(self, ts: np.ndarray, y0: float):\n",
    "        term = ODETerm(self.drift)\n",
    "        solver = Tsit5()\n",
    "        saveat = SaveAt(ts=ts, dense=True)\n",
    "        sol = diffeqsolve(\n",
    "            term, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat\n",
    "        )\n",
    "        return vmap(sol.evaluate)(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of us who have learned about ODEs, \n",
    "the structure of the code above should look pretty familiar.\n",
    "The diffrax API neatly organizes what we need to solve ODEs:\n",
    "\n",
    "- the `ODETerm`, which is the $\\frac{dy}{dt}$ equation,\n",
    "- a `solver`, for which `diffrax` provides a library of them,\n",
    "- the initial and end points $t_0$ and $t_1$ along the $t$ axis along with step size $dt$,\n",
    "- the initial value of $y$, i.e. $y_0$.\n",
    "\n",
    "Finally, when calling on the ODE,\n",
    "we evaluate the solution path from the starting time to the ending time,\n",
    "given an initial starting value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE(exponential_decay_drift)\n",
    "ts = np.linspace(0, 10, 1000)\n",
    "ys = ode(ts=ts, y0=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| fig-cap: Solution to the ODE $f'(y) = -y$.\n",
    "#| label: fig-ode-exponential-decay\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "plt.plot(ts, ys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the ODE that we had above is an exponential decay,\n",
    "and that is exactly what we see in the curve above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we wanted to run the ODE from multiple starting points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Multiple solutions to the ODE $f'(y) = -y$.\n",
    "#| label: fig-ode-multiple-decay\n",
    "\n",
    "ys = ode(ts=ts, y0=np.arange(-10, 10))\n",
    "\n",
    "for curve in ys.T:\n",
    "    plt.plot(ts, curve, color=\"blue\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Differential Equations\n",
    "\n",
    "Stochastic differential equations (SDEs) extend ODEs\n",
    "by adding in noise into each step when solving. \n",
    "SDEs can thus be thought of as having a \"drift\" component,\n",
    "in which the system being modeled by the SDE \"drifts\" through the vector field,\n",
    "and a \"diffusion\" component,\n",
    "in which the system's state is perturbed with additional noise.\n",
    "SDEs have the general form:\n",
    "\n",
    "$$dx = f(x, t)dt + g(t)dw$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To paraphrase Yang's blog post, here are the definitions of each of the terms.\n",
    "\n",
    "- $f(x, t)$ is a drift function that produces a vector output, \n",
    "  i.e. what would have been the ODE term. \n",
    "  This term controls the \"drift\"-ing of the system in observed data space.\n",
    "- $g(t)$ is a diffusion function that produces a scalar output, \n",
    "  i.e. the scalar multiplier of $dw$.\n",
    "  This term adds \"diffusive\" noise to the output.\n",
    "- $dw$ is the infinitesimal white noise term.\n",
    "\n",
    "$f(x, t)dt$ is usually referred to as the \"ODE Term\",\n",
    "while $g(t)dw$ is usually referred to as the \"Control Term\".\n",
    "We can see that in the implementation of the SDE module below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from diffrax import (\n",
    "    ControlTerm,\n",
    "    MultiTerm,\n",
    "    VirtualBrownianTree,\n",
    "    Heun\n",
    ")\n",
    "\n",
    "class SDE(eqx.Module):\n",
    "    drift: callable\n",
    "    diffusion: callable\n",
    "\n",
    "    def __call__(self, ts: np.ndarray, y0: float, key: random.PRNGKey):\n",
    "        brownian_motion = VirtualBrownianTree(ts[0], ts[-1], tol=1e-3, shape=(), key=key)\n",
    "        terms = MultiTerm(ODETerm(self.drift), ControlTerm(self.diffusion, brownian_motion))\n",
    "        solver = Heun()\n",
    "        saveat = SaveAt(t0=True, ts=ts, dense=True)\n",
    "        sol = diffeqsolve(terms, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat)\n",
    "        return vmap(sol.evaluate)(ts) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Decay\n",
    "\n",
    "For illustration, \n",
    "let's see what happens we we apply homoskedastic noise to the decay process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each instance of random noise is paired with one SDE.\n",
    "from functools import partial\n",
    "\n",
    "def homoskedastic_diffusion(t, y, args):\n",
    "    return 0.3\n",
    "\n",
    "n_timesteps = 17\n",
    "n_starting = 1001\n",
    "\n",
    "\n",
    "demo_key = random.PRNGKey(55)\n",
    "\n",
    "y0_key, key = random.split(demo_key)\n",
    "y0s = random.normal(y0_key, shape=(n_starting,))\n",
    "\n",
    "sde_keys = random.split(key, len(y0s))\n",
    "ts = np.linspace(0, 4, n_timesteps)\n",
    "sde = SDE(drift=exponential_decay_drift, diffusion=homoskedastic_diffusion)\n",
    "sde = partial(sde, ts)\n",
    "ys = vmap(sde)(y0s, sde_keys)\n",
    "for y in ys:\n",
    "    plt.plot(ts, y, alpha=0.01, color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscillating SDE \n",
    "\n",
    "Let's do one final one just to hammer home the point\n",
    "that we can generate differing marginals.\n",
    "We have a cosine drift term with homoskedastic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_drift(t, y, args):\n",
    "    return np.cos(t)\n",
    "\n",
    "\n",
    "sde = SDE(drift=cosine_drift, diffusion=homoskedastic_diffusion)\n",
    "ts_oscillating = np.linspace(1, 10, n_timesteps)\n",
    "sde = partial(sde, ts_oscillating)\n",
    "keys = random.split(key, 1001)\n",
    "oscillating_y0s = random.normal(key, shape=(1001,)) * 0.1\n",
    "oscillating_ys = vmap(sde)(oscillating_y0s, keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in oscillating_ys:\n",
    "    plt.plot(ts_oscillating, y, color=\"blue\", alpha=0.01)\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timepoint, there is also a marginal distribution.\n",
    "Let's inspect that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "fig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, t, y in zip(axes, ts_oscillating, oscillating_ys.T):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(y), bins=30)\n",
    "    plt.title(f\"time={t:.1f}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.delaxes(axes[-1])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noising SDE\n",
    "\n",
    "Another SDE that we might want is something that has increasing amounts of noise over time.\n",
    "Drift would basically be a 0 term, while the diffusion term would be some multiplier on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_drift(t, y, args):\n",
    "    \"\"\"Constant drift term.\"\"\"\n",
    "    return 0\n",
    "\n",
    "def time_dependent_diffusion(t, y, args):\n",
    "    \"\"\"Diffusion term that increases with time.\"\"\"\n",
    "    return 0.3 * t\n",
    "\n",
    "\n",
    "sde = SDE(drift=constant_drift, diffusion=time_dependent_diffusion)\n",
    "ts_noising = np.linspace(0, 4, n_timesteps)\n",
    "sde = partial(sde, ts_noising)\n",
    "y0s = random.normal(key, shape=(n_starting,)) * 0.1  # we start with N(0, 0.1) draws.\n",
    "keys = random.split(key, n_starting)\n",
    "noising_ys = vmap(sde)(y0s, keys)\n",
    "# DEBUGGING\n",
    "print(y0s.shape, ts_noising.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-noising-sde\n",
    "#| fig-cap: A \"noising\" SDE that progressively adds more noise over time.\n",
    "for y in noising_ys:\n",
    "    plt.plot(ts_noising, y, color=\"blue\", alpha=0.01)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"{n_starting} sample trajectories\")\n",
    "    sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to obtain greater amounts of noise from a tight starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "fig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, t, y in zip(axes, ts_noising, noising_ys.T):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(y), bins=30)\n",
    "    plt.title(f\"time={t:.1f}, σ={onp.std(y):.2f}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.delaxes(axes[-1])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the marginal distributions at each noise timestep,\n",
    "we see that we indeed have ever increasing amounts of noise.\n",
    "(Note how the x-axis scale is the same on all of the plots.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Time SDEs\n",
    "\n",
    "With constant drift and time-dependent diffusion, we can noise up data in a continuous fashion.\n",
    "How do we go backwards?\n",
    "Here is where solving the reverse time SDE will come in. \n",
    "Again, we need to set up the drift and diffusion terms.\n",
    "Here, the drift term is:\n",
    "\n",
    "$$f(x, t) - g^2(t) \\nabla_x \\log p_t (x) $$\n",
    "\n",
    "And the diffusion term is:\n",
    "\n",
    "$$g(t) dw$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the tricky part here is that we don't have access to\n",
    "$\\nabla_x \\log p_t (x)$ (the true score function).\n",
    "As such, we need to bring out our score model approximator!\n",
    "To train the score model approximator,\n",
    "we need the analogous score matching objective for continuous time problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal situation,\n",
    "we would train the score matching model\n",
    "using a weighted combination of Fisher divergences:\n",
    "\n",
    "$$\\mathbb{E}_{t \\in U(0, T)} \\mathbb{E}_{p_t(x)} [ \\lambda(t) || \\nabla_x \\log p_t(x) - s_{\\theta}(x, t) ||^2_2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just like before, we don't have access to $\\nabla_x \\log p_t (x)$,\n",
    "so we instead use the score matching objective by Hyvärinen.\n",
    "What's really cool here is that we can train the models using the noised up data.\n",
    "The protocol is basically as follows:\n",
    "\n",
    "1. Noise up our original data using an SDE.\n",
    "2. Train score models to estimate the score function of the noised up data.\n",
    "3. Use the approximate score function to calculate the reverse-time SDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this right, we need a GaussianModel that is compatible with SDEs,\n",
    "i.e. they accept both `x` and `t` as part of the function signature\n",
    "and return the gradient value.\n",
    "\n",
    "```python\n",
    "score: float = score_model(x, t) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd\n",
    "from jax import nn\n",
    "import equinox as eqx\n",
    "\n",
    "class SDEFeedForwardModel1D(eqx.Module):\n",
    "    r\"\"\"Time-dependent Gaussian score function.\n",
    "\n",
    "    Key trick here is to make the parameters $\\mu$ and $\\log(\\sigma)$ depend on $t$,\n",
    "    i.e. the time at which a Gaussian was sampled.\n",
    "    \"\"\"\n",
    "\n",
    "    mlp: eqx.Module\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size=2,\n",
    "        out_size=1,\n",
    "        width_size=256,\n",
    "        depth=1,\n",
    "        activation=nn.softplus,\n",
    "        key=random.PRNGKey(45),\n",
    "    ):\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=in_size,\n",
    "            out_size=out_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=activation,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x: float, t: float):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        :param x: Data. Should be of shape (1, :),\n",
    "            as the model is intended to be vmapped over batches of data.\n",
    "        :returns: Estimated score of a Gaussian.\n",
    "        \"\"\"\n",
    "        if isinstance(x, float) or x.ndim == 0:\n",
    "            x = np.array([x])\n",
    "        if isinstance(t, float) or x.ndim == 0:\n",
    "            t = np.array([t])\n",
    "        x = np.array([x.squeeze(), t.squeeze()])\n",
    "        return self.mlp(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SDEFeedForwardModel1D(width_size=256, depth=2, activation=nn.softplus, key=random.PRNGKey(55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap \n",
    "scales = time_dependent_diffusion(ts, y=None, args=None)\n",
    "\n",
    "# Test drive the model per time point.\n",
    "idx = -1\n",
    "t = ts_noising[idx]\n",
    "vmap(partial(model, t=t))(noising_ys.T[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to calculate the loss for a batch of data.\n",
    "Here, one batch of data is a set of noised up data at one particular timepoint.\n",
    "For each time point, we vmap the model partialled out at that `t`\n",
    "over the batch of data that are present there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import score_matching_loss\n",
    "def sde_score_matching_loss(model, noise_batch: np.ndarray, t: float):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: Equinox model.\n",
    "    :param noise_batch: Batch of data from 1 noise scale.\n",
    "    :param t: Time in SDE at which the noise scale was evaluated.\n",
    "    \"\"\"\n",
    "    # We need to change this such that `y0s` (arrays) \n",
    "    # are passed into the model/dmodel as `y0` (floats)\n",
    "    model = partial(model, t=t)\n",
    "    dmodel = jacfwd(model, argnums=0)\n",
    "    term1 = vmap(dmodel)(noise_batch)\n",
    "    term2 = 0.5 * vmap(model)(noise_batch) ** 2\n",
    "    term2 = np.reshape(term2, term1.shape)\n",
    "    inner_term = term1 + term2\n",
    "    summed_by_dims = vmap(np.sum)(inner_term)\n",
    "    return np.mean(summed_by_dims)\n",
    "\n",
    "sde_score_matching_loss(model, noising_ys.T[0], t=1.0)\n",
    "\n",
    "\n",
    "def joint_sde_score_matching_loss(model, data: np.ndarray, ts: np.ndarray) -> float:\n",
    "    loss = 0\n",
    "    for noise_batch, t in zip(data, ts):\n",
    "        scale = np.std(noise_batch.squeeze())\n",
    "        loss += sde_score_matching_loss(model, noise_batch, t) * scale\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from score_models.training import fit\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = SDEFeedForwardModel1D(key=random.PRNGKey(55))\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    # optax.clip(0.1),\n",
    "    optax.adam(5e-4),\n",
    ")\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "dloss = eqx.filter_jit(eqx.filter_value_and_grad(joint_sde_score_matching_loss))\n",
    "\n",
    "n_steps = 13_000\n",
    "iterator = tqdm(range(n_steps))\n",
    "loss_history = []\n",
    "updated_score_model = model\n",
    "for step in iterator:\n",
    "    loss_score, grads = dloss(updated_score_model, noising_ys.T, ts_noising)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    updated_score_model = eqx.apply_updates(updated_score_model, updates)\n",
    "    iterator.set_description(f\"Score: {loss_score:.2f}\")\n",
    "    loss_history.append(float(loss_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss Score\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-drive updated score model\n",
    "vmap(partial(updated_score_model, t=ts_noising[-1]))(noising_ys.T[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity-check: score estimators match up with alternate calculation\n",
    "\n",
    "Because we started with Gaussian noise and expanded the noise outwards,\n",
    "we still have Gaussians.\n",
    "Let's check that the scores match a Gaussian's score \n",
    "fitted onto the marginal distributions at each timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Estimated (blue) vs. approximated (red) score functions at each time evaluation. Estimated score comes from taking the location (mean) and scale (stdev) of the observed data, while approximated score comes from the time-based score model.\n",
    "#| label: fig-score-vs-time\n",
    "from jax.scipy.stats import norm \n",
    "from jax import grad \n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(8.5, 11))\n",
    "\n",
    "for t, noised_ys, ax in zip(ts_noising, noising_ys.T, axes.flatten()):\n",
    "    plt.sca(ax)\n",
    "    noised_ys_mu = np.mean(noised_ys)\n",
    "    noised_ys_sigma = np.std(noised_ys)\n",
    "\n",
    "    logp_func = partial(norm.logpdf, loc=noised_ys_mu, scale=noised_ys_sigma)\n",
    "    np.sum(logp_func(noised_ys))\n",
    "    dlogp_func = grad(logp_func)\n",
    "\n",
    "    support = np.linspace(noised_ys_mu - noised_ys_sigma * 3, noised_ys_mu + noised_ys_sigma * 3, 1000)\n",
    "    estimated_score = vmap(dlogp_func)(support)\n",
    "    approximated_score = vmap(partial(updated_score_model, t=t))(support)\n",
    "    plt.plot(support, estimated_score, color=\"blue\", label=\"estimated\")\n",
    "    plt.plot(support, approximated_score, color=\"red\", label=\"approximated\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"t={t:.2f}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.sca(axes.flatten()[0])\n",
    "plt.legend()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in @fig-score-vs-time,\n",
    "it looks like our score model is able to approximate a time-dependent score function!\n",
    "The score function is least well-approximated within the region of 2 sigmas of support,\n",
    "even if not across the full 3 sigmas.\n",
    "This is encouraging.\n",
    "We should also note that the `t=4.00` timepoint is the least well-approximated\n",
    "compared to the `t=1.00` timepoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to look at the reverse drift.\n",
    "In an SDE, the drift term dictates \n",
    "where the system is going to move towards in the next time step.\n",
    "Let's plot the vector field evaluated at each time step `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_drift(t: float, y: float, args: tuple):\n",
    "    f = constant_drift(t, y, args)\n",
    "    g = time_dependent_diffusion(t, y, args)\n",
    "    s = updated_score_model(y, t)\n",
    "    return f - 0.5 * g**2 * s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot each of these four terms to make sure we get a good feel for what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "from tqdm.auto import tqdm \n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 11))\n",
    "\n",
    "# Plot constant drift as a function of y and t.\n",
    "ys = np.linspace(-3, 3, 50)\n",
    "ts = np.linspace(0, 5, 50)\n",
    "\n",
    "function_evals = []\n",
    "for yval in tqdm(ys):\n",
    "    for t in ts:\n",
    "        dd = dict()\n",
    "        dd[\"constant_drift\"] = constant_drift(y=yval, t=t, args=())\n",
    "        dd[\"time_dependent_diffusion\"] = time_dependent_diffusion(y=yval, t=t, args=())\n",
    "        dd[\"score_approximation\"] = updated_score_model(x=yval, t=t)\n",
    "        dd[\"reverse_drift\"] = reverse_drift(y=yval, t=t, args=())\n",
    "        dd[\"y\"] = (yval)\n",
    "        dd[\"t\"] = t \n",
    "        function_evals.append(dd)\n",
    "\n",
    "columns = [\"constant_drift\", \"time_dependent_diffusion\", \"score_approximation\", \"reverse_drift\"]\n",
    "\n",
    "function_df = pd.DataFrame(function_evals)\n",
    "for column in function_df.columns:\n",
    "    function_df[column] = function_df[column].astype(float)\n",
    "\n",
    "for ax, col in zip(axes.flatten(), columns):\n",
    "    function_eval = function_df[[\"y\", \"t\", col]].pivot_table(index=\"y\", columns=\"t\", values=col)\n",
    "    sns.heatmap(function_eval, ax=ax, cmap=\"viridis\")\n",
    "    ax.set_title(col)\n",
    "    ax.set_xticklabels([f\"{float(i._text):.2f}\" for i in ax.get_xticklabels()])\n",
    "    ax.set_yticklabels([f\"{float(i._text):.2f}\" for i in ax.get_yticklabels()])\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `constant_drift` is always 0, so no problem.\n",
    "- `time_dependent_diffusion` shows how diffusion increases over time, independent of the value of `y`, which is also correct, so no problem.\n",
    "- `score_approximation` shows how the Gaussian score approximator gives a gradient that is positive-valued when `y` is negative and vice versa, which pushes us towards region of high density. Also correct.\n",
    "- `reverse_drift` shows us something interesting. We will end up with exploding values because +ve values drift more positive, while -ve values drift more -ve, until we hit very small time steps, and then we have no directional drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Flow ODEs\n",
    "\n",
    "Now that we've recapped what an ODE is, let's examine what probability flow ODEs are.\n",
    "Probability flow ODEs have the following form:\n",
    "\n",
    "$$dx = [f(x,t) - \\frac{1}{2} g^2(t) \\nabla_x \\log p_t (x)] dt$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the SDE above,\n",
    "\n",
    "$$dx = f(x, t)dt + g(t)dw$$\n",
    "\n",
    "the terms carry the same meaning:\n",
    "\n",
    "> - $f(x, t)$ is a drift function that produces a vector output,\n",
    "> - $g(t)$ is a diffusion function that produces a scalar output,\n",
    "> - and $dw$ is infinitesimal white noise.  \n",
    "> \n",
    "> (paraphrased from Yang's blog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, to obtain the appropriate ODE,\n",
    "we just need to take the score function term, square it,\n",
    "and multiply it with a score function (or an estimator function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment here with Gaussians, just to see how this works out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_term(y, t, args):\n",
    "    f = constant_drift(y, t, args)\n",
    "    g = time_dependent_diffusion(y, t, args)\n",
    "    s = updated_score_model(t, y)\n",
    "    return f - 0.5 * g**2 * s\n",
    "\n",
    "\n",
    "ode_combined = ODE(combined_term)\n",
    "\n",
    "\n",
    "ode = ode_combined\n",
    "ts = np.linspace(5, 0, 1000)\n",
    "key = random.PRNGKey(55)\n",
    "y0s = np.linspace(-5 ,5, 10)\n",
    "\n",
    "for y0 in tqdm(y0s):\n",
    "    ys = ode(ts, y0)\n",
    "    plt.plot(ts, ys)\n",
    "\n",
    "# sns.despine()\n",
    "# plt.ylim(-3, 3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
