{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Equations\n",
    "\n",
    "We're going to take a small detour through ordinary differential equations (ODEs).\n",
    "As should become evident shortly,\n",
    "this topic is important for our journey through score modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary differential equations\n",
    "\n",
    "Before we go into probability flow ODEs, though, \n",
    "we should revisit their more, ahem, _ordinary_ counterpart,\n",
    "ordinary differential equations (ODEs).\n",
    "ODEs are usually taught in undergraduate calculus classes,\n",
    "since they involve differentiation and integration.\n",
    "I do remember encountering them in high school in Singapore,\n",
    "which is a testament to how advanced the mathematics curriculum in Singapore is.\n",
    "\n",
    "ODEs are useful models of systems\n",
    "where we believe that the rate of change of an output variable\n",
    "is a math function of some input variable.\n",
    "In abstract mathematical symbols:\n",
    "\n",
    "$$\\frac{dy}{dx} = f(x, \\theta)$$\n",
    "\n",
    "Here, $f$ simply refers to some mathematical function of $x$\n",
    "and the function's parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classic ODE example\n",
    "\n",
    "A classic ODE example that we might think of is that of a decay curve:\n",
    "\n",
    "$$\\frac{dy}{dt} = -y$$\n",
    "\n",
    "Implemented in `diffrax`, which is a JAX package for differential equations,\n",
    "and wrapped in Equinox as a parameterized function,\n",
    "we have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffrax import diffeqsolve, Tsit5, ODETerm, SaveAt\n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import equinox as eqx\n",
    "\n",
    "def exponential_decay_drift(t, y, args):\n",
    "    return -y\n",
    "\n",
    "\n",
    "class ODE(eqx.Module):\n",
    "    drift: callable\n",
    "\n",
    "    def __call__(self, ts: np.ndarray, y0: float):\n",
    "        term = ODETerm(self.drift)\n",
    "        solver = Tsit5()\n",
    "        saveat = SaveAt(ts=ts, dense=True)\n",
    "        sol = diffeqsolve(term, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat)\n",
    "        return vmap(sol.evaluate)(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of us who have learned about ODEs, \n",
    "the structure of the code above should look pretty familiar.\n",
    "The diffrax API neatly organizes what we need to solve ODEs:\n",
    "\n",
    "- the `ODETerm`, which is the $\\frac{dy}{dt}$ equation,\n",
    "- a `solver`, for which `diffrax` provides a library of them,\n",
    "- the initial and end points $t_0$ and $t_1$ along the $t$ axis along with step size $dt$,\n",
    "- the initial value of $y$, i.e. $y_0$.\n",
    "\n",
    "At the end of it all we have a `sol`ution object that we can use later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the solved ODE, we can use it to plot what the solution would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE(exponential_decay_drift)\n",
    "ts = np.linspace(0, 10, 1000)\n",
    "ys = ode(ts=ts, y0=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| fig-cap: Solution to the ODE $f'(y) = -y$.\n",
    "#| label: fig-ode-exponential-decay\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "plt.plot(ts, ys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the ODE that we had above is an exponential decay,\n",
    "and that is exactly what we see in the curve above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we wanted to run the ODE from multiple starting points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Multiple solutions to the ODE $f'(y) = -y$.\n",
    "#| label: fig-ode-multiple-decay\n",
    "\n",
    "ys = ode(ts=ts, y0=np.arange(0, 10))\n",
    "\n",
    "for curve in ys.T:\n",
    "    plt.plot(ts, curve)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Differential Equations\n",
    "\n",
    "Stochastic differential equations (SDEs) extend ODEs\n",
    "by adding in noise into each step when solving. \n",
    "SDEs can thus be thought of as having a \"drift\" component,\n",
    "in which the system being modeled by the SDE \"drifts\" through the vector field,\n",
    "and a \"diffusion\" component,\n",
    "in which the system's state is perturbed with additional noise.\n",
    "Let's look at an example from diffrax's documentation,\n",
    "modified to be a bit simpler.\n",
    "We basically have:\n",
    "\n",
    "$$\\frac{dy}{dt} = -y + N(0, 0.1)$$\n",
    "\n",
    "where $N(0, 0.1)$ refers to a draw from an i.i.d. Gaussian.\n",
    "\n",
    "More generally, we have the following form:\n",
    "\n",
    "$$dx = f(x, t)dt + g(t)dw$$\n",
    "\n",
    "Here, \n",
    "\n",
    "> - $f(x, t)$ is a drift function that produces a vector output,\n",
    "> - $g(t)$ is a diffusion function that produces a scalar output,\n",
    "> - and $dw$ is infinitesimal white noise.  \n",
    "> \n",
    "> (paraphrased from Yang's blog)\n",
    "\n",
    "\n",
    "This is implemented in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from diffrax import (\n",
    "    ControlTerm,\n",
    "    MultiTerm,\n",
    "    VirtualBrownianTree,\n",
    ")\n",
    "\n",
    "class SDE(eqx.Module):\n",
    "    drift: callable\n",
    "    diffusion: callable\n",
    "\n",
    "    def __call__(self, ts: np.ndarray, y0: float, key: random.PRNGKey):\n",
    "        brownian_motion = VirtualBrownianTree(ts[0], ts[-1], tol=1e-3, shape=(), key=key)\n",
    "        terms = MultiTerm(ODETerm(self.drift), ControlTerm(self.diffusion, brownian_motion))\n",
    "        solver = Tsit5()\n",
    "        saveat = SaveAt(ts=ts, dense=True)\n",
    "        sol = diffeqsolve(terms, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat)\n",
    "        return vmap(sol.evaluate)(ts) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each instance of random noise is paired with one SDE.\n",
    "from functools import partial\n",
    "\n",
    "def homoskedastic_diffusion(t, y, args):\n",
    "    return 0.1\n",
    "\n",
    "n_timesteps = 17\n",
    "n_starting = 1001\n",
    "\n",
    "\n",
    "key = random.PRNGKey(55)\n",
    "y0s = random.normal(key, shape=(n_starting,))\n",
    "keys = random.split(key, len(y0s))\n",
    "ts = np.linspace(0, 6, n_timesteps)\n",
    "sde = SDE(drift=exponential_decay_drift, diffusion=homoskedastic_diffusion)\n",
    "sde = partial(sde, ts)\n",
    "ys = vmap(sde)(y0s, keys)\n",
    "for y in ys:\n",
    "    plt.plot(ts, y, alpha=0.01, color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noising SDE\n",
    "\n",
    "Another SDE that we might want is something that has increasing amounts of noise over time.\n",
    "Drift would basically be a 0 term, while the diffusion term would be some multiplier on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_drift(t, y, args):\n",
    "    return 0\n",
    "\n",
    "def time_dependent_diffusion(t, y, args):\n",
    "    return 0.1 * t\n",
    "\n",
    "\n",
    "sde = SDE(drift=constant_drift, diffusion=time_dependent_diffusion)\n",
    "ts = np.linspace(0, 5, n_timesteps)\n",
    "sde = partial(sde, ts)\n",
    "y0s = random.normal(key, shape=(n_starting,)) * 0.1\n",
    "keys = random.split(key, n_starting)\n",
    "noising_ys = vmap(sde)(y0s, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-noising-sde\n",
    "#| fig-cap: A \"noising\" SDE that progressively adds more noise over time.\n",
    "for y in noising_ys:\n",
    "    plt.plot(ts, y, color=\"blue\", alpha=0.01)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"{n_starting} sample trajectories\")\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to obtain greater amounts of noise from a tight starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timepoint, there is also a marginal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "fig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, t, y in zip(axes, ts, noising_ys.T):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(y), bins=30)\n",
    "    plt.title(f\"time={t:.1f}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.delaxes(axes[-1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscillating SDE \n",
    "\n",
    "Let's do one final one just to hammer home the point.\n",
    "We have a cosine drift term with homoskedastic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_drift(t, y, args):\n",
    "    return np.sin(t)\n",
    "\n",
    "sde = SDE(drift=cosine_drift, diffusion=homoskedastic_diffusion)\n",
    "ts = np.linspace(1, 10, n_timesteps)\n",
    "sde = partial(sde, ts)\n",
    "keys = random.split(key, 1001)\n",
    "oscillating_y0s = random.normal(key, shape=(1001,)) * 0.1\n",
    "oscillating_ys = vmap(sde)(oscillating_y0s, keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in oscillating_ys:\n",
    "    plt.plot(ts, y, color=\"blue\", alpha=0.01)\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "fig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, t, y in zip(axes, ts, oscillating_ys.T):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(y), bins=30)\n",
    "    plt.title(f\"time={t:.1f}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.delaxes(axes[-1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Time SDEs\n",
    "\n",
    "With constant drift and time-dependent diffusion, we can noise up data in a continuous fashion.\n",
    "How do we go backwards?\n",
    "Here is where solving the reverse time SDE will come in. \n",
    "Again, we need to set up the drift and idifusion terms.\n",
    "Here, drift is:\n",
    "\n",
    "$$f(x, t) - g^2(t) \\nabla_x \\log p_t (x) $$\n",
    "\n",
    "And diffusion is:\n",
    "\n",
    "$$g(t) dw$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the tricky part here is that we don't have access to\n",
    "$\\nabla_x \\log p_t (x)$ (the true score function).\n",
    "As such, we need to bring out our score model approximator!\n",
    "To train the score model approximator,\n",
    "we need the analogous score matching objective for continuous time problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal situation,\n",
    "we would train the score matching model\n",
    "using a weighted combination of Fisher divergences:\n",
    "\n",
    "$$\\mathbb{E}_{t \\in U(0, T)} \\mathbb{E}_{p_t(x)} [ \\lambda(t) || \\nabla_x \\log p_t(x) - s_{\\theta}(x, t) ||^2_2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just like before, we don't have access to $\\nabla_x \\log p_t (x)$,\n",
    "so we instead use the score matching objective by Hyvärinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models.feedforward import FeedForwardModel1D\n",
    "\n",
    "model = FeedForwardModel1D(width_size=256, depth=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_drift(y, t, args):\n",
    "    t = np.clip(t, a_min=0.001)\n",
    "    f = np.asarray(constant_drift(y, t, args))\n",
    "    g = np.asarray(time_dependent_diffusion(y, t, args))\n",
    "    gaussian_score = GaussianModel(mu=0, log_sigma=0.1 * np.log(t))\n",
    "    s = np.asarray(gaussian_score(y))\n",
    "    return f - 0.5 * g**2 * s\n",
    "\n",
    "reverse_drift(y=3.0, t=0.0, args=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noising_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sde = SDE(reverse_drift, time_dependent_diffusion)\n",
    "ts = np.linspace(5, 0, n_timesteps)  # key here: time has to be reversed!\n",
    "sde = partial(sde, ts)\n",
    "keys = random.split(random.PRNGKey(45), n_starting)\n",
    "y0s = vmap(sde)(noising_ys.T[-1], keys)\n",
    "\n",
    "for y in y0s:\n",
    "    plt.plot(ts, y, color=\"blue\", alpha=0.1)\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.gca().invert_xaxis()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes here: \n",
    "\n",
    "- In computational experiments, I noticed under/overflow issues with certain parameter scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Flow ODEs\n",
    "\n",
    "Now that we've recapped what an ODE is, let's examine what probability flow ODEs are.\n",
    "Probability flow ODEs have the following form:\n",
    "\n",
    "$$dx = [f(x,t) - \\frac{1}{2} g^2(t) \\nabla_x \\log p_t (x)] dt$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the SDE above,\n",
    "\n",
    "$$dx = f(x, t)dt + g(t)dw$$\n",
    "\n",
    "the terms carry the same meaning:\n",
    "\n",
    "> - $f(x, t)$ is a drift function that produces a vector output,\n",
    "> - $g(t)$ is a diffusion function that produces a scalar output,\n",
    "> - and $dw$ is infinitesimal white noise.  \n",
    "> \n",
    "> (paraphrased from Yang's blog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here, to obtain the appropriate ODE,\n",
    "we just need to take the score function term, square it,\n",
    "and multiply it with a score function (or an estimator function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment here with Gaussians, just to see how this works out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_term(y, t, args):\n",
    "    f = constant_drift(y, t, args)\n",
    "    g = time_dependent_diffusion(y, t, args)\n",
    "    gaussian_score = GaussianModel(mu=0, log_sigma=t)\n",
    "    s = gaussian_score(y)\n",
    "    return f - 0.5 * g**2 * s\n",
    "\n",
    "\n",
    "ode_combined = ODE(reverse_drift)\n",
    "# ode_drift = ODE(constant_drift)\n",
    "# ode_diffusion = ODE(lambda y, t, args: -time_dependent_diffusion(y, t, args))\n",
    "\n",
    "\n",
    "ode = ode_combined\n",
    "ts = np.linspace(5, 0, 1000)\n",
    "key = random.PRNGKey(55)\n",
    "y0s = random.normal(key=key, shape=(100,))\n",
    "ys = ode(ts, y0s)\n",
    "\n",
    "for y in ys.T:\n",
    "    plt.plot(ts, y, color=\"blue\", alpha=0.1)\n",
    "\n",
    "sns.despine()\n",
    "plt.ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
