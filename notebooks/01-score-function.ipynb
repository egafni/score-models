{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I need to wrap my head around\n",
    "is what the score function is\n",
    "and how we estimate it from data.\n",
    "I will use the venerable Gaussian to anchor my understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the score function\n",
    "is the derivative of the log density of a probability distribution\n",
    "w.r.t. the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad, random, vmap\n",
    "from jax.scipy.stats import norm\n",
    "from jax.tree_util import Partial as partial\n",
    "import matplotlib.pyplot as plt\n",
    "from jaxopt import GradientDescent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 1000)\n",
    "y = norm.logpdf(x, loc=0, scale=1)\n",
    "\n",
    "model_score = grad(norm.logpdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that at the top of the Gaussian,\n",
    "the gradient should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the tails, the gradient should be of higher magnitude\n",
    "than at the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score(-3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score(3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we have data\n",
    "but don't know the parameters of the true data-generating density?\n",
    "In this case, we need to estimate the score function,\n",
    "which means estimating the parameters of the model.\n",
    "To do this, I will lean on work by [Aapo Hyvärinen from 2005 in JMLR][jmlr2005].\n",
    "In that work, Hyvärinen proposes \n",
    "to estimate the parameters of the data-generating density:\n",
    "\n",
    "> by minimizing the expected squared distance between the model score function\n",
    "> $\\psi(.;\\theta)$ and the data score function $\\psi_x(.)$.\n",
    "\n",
    "[jmlr2005]: https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This squared distance is defined as the function $J(\\theta)$,\n",
    "where $\\theta$ are the parameters of the data-generating model.\n",
    "For a finite sample, Hyvärinen provides an exact formula that we can try to implement\n",
    "in Python/JAX NumPy.\n",
    "\n",
    "> $\\tilde{J}(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i (x(t); \\theta)^2 ] + \\text{const.}$\n",
    "\n",
    "That's one heck of a complicated formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all formulas, we need the definitions.\n",
    "\n",
    "- $i$ _probably_ is an indexer into dimensions \n",
    "  for a multidimensional probability distribution.\n",
    "- $\\psi_i$ is the data score function for a finite sample.\n",
    "- $\\delta_i \\psi_i$ is the gradient of the data score function, $\\psi_i$.\n",
    "  Yes, you heard right, we need the derivative of a derivative, i.e. the 2nd derivative!\n",
    "- $\\theta$ are the parameters of the score function. \n",
    "  If the score function comes from the logpdf of a Gaussian,\n",
    "  then $\\mu$ and $\\sigma$ are the parameters;\n",
    "  if the score function is a neural network approximation,\n",
    "  then $\\theta$ refer to the parameters of the neural network.\n",
    "- $x(t)$ are the observed samples of data. \n",
    "  As the outer $\\frac{1}{T} \\sum_{t=1}^{T}$ suggests, \n",
    "  we will need to do a mean over observed samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample some data from a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(44)\n",
    "\n",
    "true_mu = 3.0\n",
    "true_sigma = 1.0\n",
    "data = random.normal(key, shape=(1000,)) * true_sigma + true_mu\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean(), data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the score of the data _under the true model_.\n",
    "We ensure that the resulting score function has the same signature\n",
    "as the underlying distribution that it is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import gaussian_model\n",
    "\n",
    "init_fun, apply_fun = gaussian_model()\n",
    "true_params = (true_mu, np.log(true_sigma))\n",
    "\n",
    "(\n",
    "    apply_fun(true_params, true_mu),\n",
    "    apply_fun(true_params, true_mu + true_sigma),\n",
    "    apply_fun(true_params, true_mu - true_sigma * 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the true data score per draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to pass in log of true_sigma!!!\n",
    "true_data_score = vmap(partial(apply_fun, true_params))(data)\n",
    "true_data_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent to find parameters of the Gaussian\n",
    "that minimize score function loss.\n",
    "To do this, we will use the GradientDescent solver from `jaxopt`,\n",
    "which will give us a really concise syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2, k3 = random.split(key, 3)\n",
    "_, params_init = init_fun(k1)\n",
    "params_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func = partial(apply_fun, params_init)\n",
    "dscore_func = grad(score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import l2_norm\n",
    "from jax import grad\n",
    "from typing import Callable\n",
    "\n",
    "def score_matching_loss(params, score_func, batch):\n",
    "    score_func = partial(score_func, params)\n",
    "    dscore_func = grad(score_func)\n",
    "\n",
    "    term1 = vmap(dscore_func)(batch)\n",
    "    term2 = (0.5 * vmap(score_func)(batch) ** 2)\n",
    "\n",
    "    inner_term = term1 + term2\n",
    "    return np.mean(inner_term).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_matching_loss(params_init, apply_fun, data)\n",
    "myloss = partial(score_matching_loss, score_func=apply_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = GradientDescent(fun=myloss, maxiter=20000, stepsize=5e-2)\n",
    "result = solver.run(params_init, batch=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the resulting params match up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_sigma = result.params\n",
    "mu, np.exp(log_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data), np.std(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like they do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_eq_x(x, y, ax):\n",
    "    minval = min(min(x), min(y))\n",
    "    maxval = max(max(x), max(y))\n",
    "\n",
    "    ax.plot([minval, maxval], [minval, maxval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_mu, est_log_sigma = result.params\n",
    "\n",
    "est_mu - true_mu, np.exp(est_log_sigma) - true_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = vmap(partial(apply_fun, (result.params)))(data)\n",
    "plt.scatter(true_data_score, model_scores)\n",
    "y_eq_x(true_data_score, model_scores, plt.gca())\n",
    "plt.xlabel(\"True Data Score\")\n",
    "plt.ylabel(\"Model Score\")\n",
    "plt.title(\"Gaussian Model Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to approximate the score function with a neural network instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Score Function with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from jax.example_libraries import stax \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import nn_model\n",
    "init_fun, apply_fun = nn_model()\n",
    "\n",
    "def score_fun(params, batch):\n",
    "    out = apply_fun(params, batch).squeeze()\n",
    "    return out \n",
    "\n",
    "\n",
    "_, params_init = init_fun(rng=random.PRNGKey(44), input_shape=(1,))\n",
    "myloss = partial(score_matching_loss, score_func=score_fun)\n",
    "\n",
    "solver = GradientDescent(fun=myloss, maxiter=1200)\n",
    "result = solver.run(params_init, batch=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = vmap(partial(apply_fun, result.params))(data).squeeze()\n",
    "plt.scatter(true_data_score, model_scores)\n",
    "y_eq_x(true_data_score, model_scores, plt.gca())\n",
    "plt.title(\"Trained Neural Network\")\n",
    "plt.xlabel(\"True Data Score\")\n",
    "plt.ylabel(\"Model Data Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = vmap(partial(apply_fun, params_init))(data).squeeze()\n",
    "plt.scatter(true_data_score, model_scores)\n",
    "y_eq_x(true_data_score, model_scores, plt.gca())\n",
    "plt.title(\"Initializsed Neural Network\")\n",
    "plt.xlabel(\"True Data Score\")\n",
    "plt.ylabel(\"Model Data Score\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a062a7e1adbb829192b5a56a463e7bc3d2201a3b05feadd05a7a64f9805fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
