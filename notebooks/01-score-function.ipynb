{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the coolest new developments in generative modelling\n",
    "is the use of _score models_ to generate samples\n",
    "that look like an existing collection of samples.\n",
    "Pioneering this work is Yang Song,\n",
    "who did this work while a PhD student at Stanford University.\n",
    "The way I first uncovered his work is through Twitter,\n",
    "where I was brought to his excellently written [blog post][yangblog] on the topic.\n",
    "\n",
    "[yangblog]: https://yang-song.github.io/blog/2021/score/\n",
    "\n",
    "In this collection of notebooks,\n",
    "I would like to explore the fundamental ideas that underlie his work.\n",
    "Along the way, we will work towards\n",
    "a pedagogical implementation of score models as generative models.\n",
    "By the end of this journey,\n",
    "we should have a much better understanding of score models and their core concepts,\n",
    "and should also have a framework for writing the code necessary\n",
    "to implement score models in JAX and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need some basic knowledge and terminology established first,\n",
    "otherwise, the terminology may become overwhelming,\n",
    "especially for those who are not well-versed in probabilistic modelling.\n",
    "As such, we're going to start with a bunch of definitions.\n",
    "Don't skip these, they're important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a score function?\n",
    "The score function is defined as follows:\n",
    "\n",
    "> The score function is\n",
    "> the gradient of the log of the probability density function \n",
    "> of a probability distribution\n",
    "> with respect to the distribution's support.\n",
    "\n",
    "There's a lot to unpack in there, \n",
    "so let's dissect the anatomy of this definition bit by bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distributions\n",
    "\n",
    "Probability distributions are super cool objects in stats[^bayes].\n",
    "Distributions can be **configured** through their parameters;\n",
    "for example, by setting the values $\\mu$ and $\\sigma$ of a Gaussian respectively.\n",
    "We can use probability distributions to generate data, \n",
    "and we can use them to evaluate the likelihood of observed data.\n",
    "The latter point is done by using a probability distribution's\n",
    "**probability density function**[^discrete].\n",
    "\n",
    "[^bayes]: I've explored the anatomy of a probability distribution\n",
    "in my essay on [Bayesian and computational statistics][bayes],\n",
    "and would recommend looking at it for a refresher.\n",
    "\n",
    "[^discrete]: Or the probability mass function, for discrete distributions,\n",
    "but we're going to stick with continuous distributions for this essay.\n",
    "\n",
    "[bayes]: https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Function\n",
    "\n",
    "A distribution's probability density function (PDF)\n",
    "describes the propensity of a probability distribution\n",
    "to generate draws of a particular value.\n",
    "As mentioned above, we primarily use the PDF to\n",
    "_evaluate the likelihood of the observing data, given the distribution's configuration_.\n",
    "If you need an anchoring example, \n",
    "think of the venerable Gaussian probability density function in @fig-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every distribution has a **support**,\n",
    "which is the range of values for which the probability distribution is defined.\n",
    "The Gaussian has support in the range $(-\\infty, \\infty)$,\n",
    "while positive-only distributions (such as the Exponential)\n",
    "have support in the range $(0, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log PDF\n",
    "\n",
    "Because the PDF is nothing more than a math function, we can take its logarithm!\n",
    "In computational statistics, taking the log is usually done for pragmatic purposes,\n",
    "as we usually end up with underflow issues otherwise.\n",
    "For the standard Gaussian above, its log PDF looks like what we see in @fig-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often call the log PDF **logp** for short,\n",
    "and in the probabilistic programming language PyMC,\n",
    "`logp` is the name of the class method use for calculating \n",
    "the log likelihood of data under the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "Finally, we get to the **score**.\n",
    "As it turns out, because the logp function is differentiable,\n",
    "we can take its derivative easily (using JAX, for example).\n",
    "The derivative of the logp function is called the **score function**.\n",
    "The score of a distribution is the gradient of the logp function w.r.t. the support.\n",
    "You can visualize what it is like in @fig-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-likelihood\n",
    "#| fig-cap: \"$P(x)$ (likelihood, PDF), $log P(x)$ (log likelihood, logp), and $dlogP(x)$ (score) of a Gaussian.\"\n",
    "from jax.scipy.stats import norm\n",
    "from jax import grad, vmap\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig, axes = plt.subplots(figsize=(8, 3), nrows=1, ncols=3, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "x = np.linspace(-3.5, 3.5, 1000)\n",
    "y = norm.pdf(x)\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "plt.title(\"PDF\")\n",
    "sns.despine()\n",
    "\n",
    "y = norm.logpdf(x)\n",
    "plt.sca(axes[1])\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.ylabel(\"logP(x)\")\n",
    "plt.title(\"Log PDF\")\n",
    "\n",
    "# Tangent Line\n",
    "def line(x):\n",
    "    return grad(norm.logpdf)(x_pt) * (x - x_pt) + y_pt\n",
    "x_pt = -1.5\n",
    "y_pt = norm.logpdf(x_pt)\n",
    "xrange = np.linspace(x_pt - 1, x_pt + 1, 10)\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.scatter(x_pt, y_pt, color=\"gray\")\n",
    "plt.plot(xrange, vmap(line)(xrange), color=\"gray\", ls=\"--\")\n",
    "\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.plot(x, vmap(grad(norm.logpdf))(x), color=\"black\")\n",
    "plt.axhline(y=0, ls=\"--\", color=\"black\")\n",
    "plt.axvline(x=0, ls=\"--\", color=\"black\")\n",
    "plt.title(\"Score\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In JAX, obtaining the score function is relatively easy.\n",
    "We simply need to use JAX's `grad` function to obtain the transformed logp function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "y = norm.logpdf(x, loc=0, scale=1)\n",
    "\n",
    "gaussian_score = grad(norm.logpdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visual inspection above,\n",
    "we know that at the top of the Gaussian,\n",
    "the gradient should be zero,\n",
    "and can verify as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_score(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the tails, the gradient should be of higher magnitude\n",
    "than at the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_score(-3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_score(3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the score function\n",
    "\n",
    "As it turns out, there is [a paper][hyvarinen]\n",
    "published by Aapo Hyv√§rinen in the 2005 in the Journal of Machine Learning Research\n",
    "that details how to _estimate_ the score function\n",
    "in the absence of knowledge of the true data generating distribution.\n",
    "When I first heard of the idea, I thought it was crazy --\n",
    "crazy cool that we could even do this!\n",
    "\n",
    "[hyvarinen]: https://jmlr.csail.mit.edu/papers/volume6/hyvarinen05a/old.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key equation in the paper is equation #4.\n",
    "This equation details how we can use an arbitrary function, $\\psi(x, \\theta)$,\n",
    "to approximate the score function,\n",
    "and the loss function needed to train the parameters of the function $\\theta$\n",
    "to approximate the score function.\n",
    "I've replicated the equation below,\n",
    "alongside a bullet-point explanation of what each of the terms are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i(x(t); \\theta)^2 ] + \\text{const}$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $J(\\theta)$ is the loss function that we wish to minimize w.r.t. the parameters $\\theta$\n",
    "- $\\theta$ are the parameters of the function $\\psi_i$\n",
    "- $\\psi_i(x(t); \\theta)$ are the score function estimators for each dimension $i$ in $x$, which has parameters $\\theta$.\n",
    "- $x(t)$ are the i.i.d. samples from the unknown data-generating distribution.\n",
    "- $\\delta_i$ refers to the partial derivative w.r.t. dimension $i$ in $x$.\n",
    "- $\\text{const}$ is a constant term that effectively can be ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the idea in a bit more detail.\n",
    "What we're going to do here is use a simple feed-forward neural network\n",
    "as the score function estimator $\\psi(x(t), \\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(44)\n",
    "\n",
    "true_mu = 3.0\n",
    "true_sigma = 1.0\n",
    "data = random.normal(key, shape=(1000,)) * true_sigma + true_mu\n",
    "data[0:10]  # showing just the first 10 samples drawn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also verify that the $\\mu$ and $\\sigma$ of the data \n",
    "are as close to the ground truth as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean(), data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the score of the data _under the true model_.\n",
    "We ensure that the resulting score function has the same signature\n",
    "as the underlying distribution that it is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import gaussian_model\n",
    "\n",
    "init_fun, apply_fun = gaussian_model()\n",
    "true_params = (true_mu, np.log(true_sigma))\n",
    "\n",
    "(\n",
    "    apply_fun(true_params, true_mu),\n",
    "    apply_fun(true_params, true_mu + true_sigma),\n",
    "    apply_fun(true_params, true_mu - true_sigma * 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the true data score per draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to pass in log of true_sigma!!!\n",
    "true_data_score = vmap(partial(apply_fun, true_params))(data)\n",
    "true_data_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent to find parameters of the Gaussian\n",
    "that minimize score function loss.\n",
    "To do this, we will use the GradientDescent solver from `jaxopt`,\n",
    "which will give us a really concise syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2, k3 = random.split(key, 3)\n",
    "_, params_init = init_fun(k1)\n",
    "params_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func = partial(apply_fun, params_init)\n",
    "dscore_func = grad(score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import l2_norm\n",
    "from jax import grad\n",
    "from typing import Callable\n",
    "\n",
    "def score_matching_loss(params, score_func, batch):\n",
    "    score_func = partial(score_func, params)\n",
    "    dscore_func = grad(score_func)\n",
    "\n",
    "    term1 = vmap(dscore_func)(batch)\n",
    "    term2 = (0.5 * vmap(score_func)(batch) ** 2)\n",
    "\n",
    "    inner_term = term1 + term2\n",
    "    return np.mean(inner_term).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_matching_loss(params_init, apply_fun, data)\n",
    "myloss = partial(score_matching_loss, score_func=apply_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = GradientDescent(fun=myloss, maxiter=20000, stepsize=5e-2)\n",
    "result = solver.run(params_init, batch=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the resulting params match up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_sigma = result.params\n",
    "mu, np.exp(log_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data), np.std(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like they do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_eq_x(x, y, ax):\n",
    "    minval = min(min(x), min(y))\n",
    "    maxval = max(max(x), max(y))\n",
    "\n",
    "    ax.plot([minval, maxval], [minval, maxval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_mu, est_log_sigma = result.params\n",
    "\n",
    "est_mu - true_mu, np.exp(est_log_sigma) - true_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_scores = vmap(partial(apply_fun, (result.params)))(data)\n",
    "plt.scatter(true_data_score, gaussian_scores)\n",
    "y_eq_x(true_data_score, gaussian_scores, plt.gca())\n",
    "plt.xlabel(\"True Data Score\")\n",
    "plt.ylabel(\"Model Score\")\n",
    "plt.title(\"Gaussian Model Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to approximate the score function with a neural network instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Score Function with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from jax.example_libraries import stax \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import nn_model\n",
    "init_fun, apply_fun = nn_model()\n",
    "\n",
    "def score_fun(params, batch):\n",
    "    out = apply_fun(params, batch).squeeze()\n",
    "    return out \n",
    "\n",
    "\n",
    "_, params_init = init_fun(rng=random.PRNGKey(44), input_shape=(1,))\n",
    "myloss = partial(score_matching_loss, score_func=score_fun)\n",
    "\n",
    "solver = GradientDescent(fun=myloss, maxiter=1200)\n",
    "result = solver.run(params_init, batch=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_scores = vmap(partial(apply_fun, result.params))(data).squeeze()\n",
    "plt.scatter(true_data_score, gaussian_scores)\n",
    "y_eq_x(true_data_score, gaussian_scores, plt.gca())\n",
    "plt.title(\"Trained Neural Network\")\n",
    "plt.xlabel(\"True Data Score\")\n",
    "plt.ylabel(\"Model Data Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_scores = vmap(partial(apply_fun, params_init))(data).squeeze()\n",
    "plt.scatter(true_data_score, gaussian_scores)\n",
    "y_eq_x(true_data_score, gaussian_scores, plt.gca())\n",
    "plt.title(\"Initializsed Neural Network\")\n",
    "plt.xlabel(\"True Data Score\")\n",
    "plt.ylabel(\"Model Data Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure this works with mixture distributions\n",
    "\n",
    "Mixture distributions are what our data will look the most like.\n",
    "Let's make sure our approximate score function \n",
    "can approximate the mixture distribution scores as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "mus = np.array([-3, 3])\n",
    "sigmas = np.array([1, 1])\n",
    "ws = np.array([0.1, 0.9])\n",
    "\n",
    "def mixture_pdf(x, mus, sigmas, ws):\n",
    "    component_pdfs = vmap(partial(norm.pdf, x))(mus, sigmas)  # 2, n_draws)\n",
    "    scaled_component_pdfs = vmap(np.multiply)(component_pdfs, ws)\n",
    "    total_pdf = np.sum(scaled_component_pdfs, axis=0)\n",
    "    return total_pdf\n",
    "\n",
    "def mixture_logpdf(x, mus, sigmas, ws):\n",
    "    return np.log(mixture_pdf(x, mus, sigmas, ws))\n",
    "\n",
    "dmixture_logpdf = grad(mixture_logpdf, argnums=0)\n",
    "mixture_logpdf_grads = vmap(partial(dmixture_logpdf, mus=mus, sigmas=sigmas, ws=ws))(x)\n",
    "plt.plot(x, mixture_logpdf_grads)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make sure that our neural network model\n",
    "is able to approximate the score function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp \n",
    "\n",
    "\n",
    "draws = 1000\n",
    "mix1 = random.normal(k1, shape=(1000,)) * 1 - 3\n",
    "mix2 = random.normal(k2, shape=(9000,)) * 1 + 3\n",
    "data = np.concatenate([mix1, mix2])\n",
    "plt.hist(onp.array(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, params_init = init_fun(rng=random.PRNGKey(44), input_shape=(1,))\n",
    "myloss = partial(score_matching_loss, score_func=score_fun)\n",
    "\n",
    "solver = GradientDescent(fun=myloss, maxiter=12000)\n",
    "result = solver.run(params_init, batch=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-6, 6, 1000)\n",
    "out = apply_fun(result.params, inputs=xs.reshape(-1, 1))\n",
    "\n",
    "plt.plot(xs, out.squeeze(), label=\"NN Estimated\")\n",
    "plt.plot(x, mixture_logpdf_grads, label=\"Ground Truth\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Estimated vs. Truth Score Function\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, we can estimate the gradient in the regime where we have lots of data!\n",
    "However, we can't seem to say the same for the tails (and midpoint of the mixture)\n",
    "where there is very little data present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm with 3-component mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.array([-7, -2, 3])\n",
    "sigmas = np.ones(3)\n",
    "ws = np.array([0.1, 0.4, 0.5])\n",
    "mixture_logpdf_grads = vmap(partial(dmixture_logpdf, mus=mus, sigmas=sigmas, ws=ws))(x)\n",
    "plt.plot(x, mixture_logpdf_grads)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"3-Component Mixture Score\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = 1000\n",
    "mix1 = random.normal(k1, shape=(1000,)) * 1 - 6\n",
    "mix2 = random.normal(k2, shape=(4000,)) * 1 - 2\n",
    "mix3 = random.normal(k3, shape=(5000,)) * 1 + 1\n",
    "data = np.concatenate([mix1, mix2, mix3])\n",
    "plt.hist(onp.array(data), bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, params_init = init_fun(rng=random.PRNGKey(44), input_shape=(1,))\n",
    "myloss = partial(score_matching_loss, score_func=score_fun)\n",
    "\n",
    "solver = GradientDescent(fun=myloss, maxiter=12000, tol=1e-4)\n",
    "result = solver.run(params_init, batch=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-10, 8, 1000)\n",
    "out = apply_fun(result.params, inputs=xs.reshape(-1, 1))\n",
    "\n",
    "plt.plot(xs, out.squeeze(), label=\"NN Estimated\")\n",
    "plt.plot(x, mixture_logpdf_grads, label=\"Ground Truth\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()  \n",
    "plt.title(\"Estimated vs. Truth Score Function\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, this actually works!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a062a7e1adbb829192b5a56a463e7bc3d2201a3b05feadd05a7a64f9805fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
