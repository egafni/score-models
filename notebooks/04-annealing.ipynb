{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Annealing\n",
    "\n",
    "We're going to do noise annealing once more, just this time in 2D.\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.utils import generate_mixture_2d\n",
    "from jax import random , numpy as np\n",
    "import jax \n",
    "\n",
    "# jax.config.update('jax_platform_name', 'cpu')  # this notebook's code doesn't fit in 8GB of GPU memory!\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "data, k3 = generate_mixture_2d(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scales = np.linspace(1, 5, 20)  # 20 different noise scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the original data and progressively noise it up.\n",
    "This is a `lax.scan` operation, not a `vmap` operation;\n",
    "the second noising depends on the first,\n",
    "the third depends on the second, etc.\n",
    "\n",
    "TODO: In my first notebook (`noise-scales.ipynb`),\n",
    "I implemented noising incorrectly, I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap , lax\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "k4, k5, k6 = random.split(k3, 3)\n",
    "\n",
    "# The draws to add are definitely independently drawn.\n",
    "\n",
    "def noise_up_one(key, sample, noise_scales):\n",
    "\n",
    "    def noise_up(prev_x, draw):\n",
    "        \"\"\"A function to noise up existing data.\"\"\"\n",
    "        new_x = prev_x + draw\n",
    "        return new_x, prev_x \n",
    "\n",
    "    draw_keys = random.split(key, len(noise_scales))\n",
    "    covs = vmap(lambda x: np.eye(len(sample)) * x)(noise_scales)\n",
    "    draws = vmap(partial(random.multivariate_normal, mean=np.zeros(len(sample))))(draw_keys, cov=covs)\n",
    "    _, noised = lax.scan(noise_up, sample, draws)\n",
    "    return noised\n",
    "\n",
    "# Test-drive\n",
    "out = noise_up_one(k4, data[0], noise_scales)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_keys = random.split(k5, len(data))\n",
    "\n",
    "noised_up_data = vmap(partial(noise_up_one, noise_scales=noise_scales))(sample_keys, data)\n",
    "noised_up_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Plot distribution over noise scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(figsize=(20, 16), nrows=4, ncols=5, sharex=True, sharey=True)\n",
    "for (scale_index, noised_data), ax in zip(enumerate(noised_up_data.swapaxes(0, 1)), axes.flatten()):\n",
    "    ax.scatter(noised_data[:, 0], noised_data[:, 1])\n",
    "    ax.set_title(f\"Noise scale: {noise_scales[scale_index]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointly train\n",
    "\n",
    "We now jointly train all 20 neural networks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import nn_model\n",
    "init_fun, nn_score_func = nn_model(output_dim=2)\n",
    "keys = random.split(key, len(noise_scales))\n",
    "key = random.split(keys[-1])[0]\n",
    "_, params_init = vmap(partial(init_fun, input_shape=(None, 2)))(keys)  # n sets of params now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test output\n",
    "\n",
    "Should be of shape (None, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import score_matching_loss\n",
    "from typing import Callable\n",
    "\n",
    "def multi_scale_loss(params, score_func: Callable, batch: np.ndarray, scales: np.ndarray):\n",
    "    \"\"\"Joint loss function for multi-scale loss.\n",
    "    \n",
    "    :param params: `vmap`-able collection of score functions.\n",
    "        Should be of length equal to the number of scales used.\n",
    "    :param score_func: A function that estimates the score of a data point.\n",
    "        It is `vmap`-ed over `params`.\n",
    "    :param batch: A collection of data points sampled from the noised-up distribution.\n",
    "        Should be of shape (n_observations, n_scales, n_dims).\n",
    "    \"\"\"\n",
    "    batch = batch.swapaxes(0, 1)\n",
    "    lossfunc = partial(score_matching_loss, score_func=score_func)  # already includes l2 norm on params\n",
    "    loss = vmap(lossfunc)(params, batch=batch)\n",
    "    scaled_loss = loss * scales \n",
    "    return np.sum(scaled_loss) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_scale_loss(params_init, nn_score_func, noised_up_data, noise_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxopt import GradientDescent\n",
    "from jax import jit \n",
    "\n",
    "joint_loss_func = jit(partial(multi_scale_loss, score_func=nn_score_func))\n",
    "solver = GradientDescent(joint_loss_func, maxiter=1000)\n",
    "result = solver.run(params_init, batch=noised_up_data, scales=noise_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_up_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learned gradient field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 21\n",
    "xs = np.linspace(-30, 30, n_points)\n",
    "ys = np.linspace(-30, 30, n_points)\n",
    "xxs, yys = np.meshgrid(xs, ys)\n",
    "xxs.shape, yys.shape\n",
    "\n",
    "x_y_pair = np.vstack([xxs.flatten(), yys.flatten()]).T\n",
    "x_y_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm \n",
    "from jax.tree_util import tree_map\n",
    "def get_params(params, i):\n",
    "    param = tree_map(lambda x: x[i], params)\n",
    "    return param\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 16), nrows=4, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "for i in tqdm(range(len(noise_scales))):\n",
    "    param = get_params(result.params, i) \n",
    "    gradient_field = vmap(partial(nn_score_func, param))(x_y_pair)\n",
    "\n",
    "    for xy_pair, vect in zip(x_y_pair, gradient_field):\n",
    "        axes.flatten()[i].arrow(*xy_pair, *vect, width=0.3, alpha=0.1)    \n",
    "    axes.flatten()[i].scatter(*noised_up_data[:, i, :].T, alpha=0.1, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm sampling from score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map\n",
    "from score_models.sampler import langevin_dynamics\n",
    "\n",
    "\n",
    "# Get out the params independently\n",
    "fig, axes = plt.subplots(figsize=(16, 16), nrows=5, ncols=4, sharex=True, sharey=True)\n",
    "k6, k7 = random.split(k5)\n",
    "keys = random.split(k6, len(noise_scales))\n",
    "for i, scale in tqdm(enumerate(noise_scales)):\n",
    "    param = tree_map(lambda x: x[i], result.params)\n",
    "    initial_states, final_states, chain_samples_joint = langevin_dynamics(\n",
    "        n_chains=200, \n",
    "        n_samples=1000, \n",
    "        key=keys[i], \n",
    "        epsilon=5e-3, \n",
    "        score_func=nn_score_func, \n",
    "        params=param,\n",
    "        init_scale=3, \n",
    "        sample_shape=(None, 2)\n",
    "    )\n",
    "    final_states = np.clip(final_states, -30, 30)\n",
    "    axes.flatten()[i].scatter(noised_up_data[:, i, 0], noised_up_data[:, i, 1], label=\"Data\", color=\"blue\", alpha=0.1)\n",
    "    axes.flatten()[i].scatter(final_states[:, 0], final_states[:, 1], label=f\"Perturbation: {scale:.2f}\", color=\"black\", alpha=0.1)\n",
    "    axes.flatten()[i].legend()\n",
    "    axes.flatten()[i].set_title(f\"Perturbation: {scale:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealed Langevin Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(20, 16), nrows=5, ncols=4, sharex=True, sharey=True)\n",
    "n_chains = 1000\n",
    "n_samples = 1000\n",
    "k8, k9, k10 = random.split(k7, 3)\n",
    "starter_xs = random.normal(k8, shape=(n_chains, 2))\n",
    "sampler_starter_xs_record = [starter_xs]\n",
    "chain_samples_record = []\n",
    "epsilon = 5e-3\n",
    "kk, kk_ = random.split(k9)\n",
    "# We start first by sampling from the \n",
    "for i, scale in tqdm(enumerate(noise_scales[::-1])):\n",
    "    kk, kk_ = random.split(kk_)\n",
    "    param = tree_map(lambda x: x[-i], result.params)\n",
    "    final_states, starter_xs, chain_samples_annealed = langevin_dynamics(\n",
    "        n_chains=n_chains, \n",
    "        n_samples=n_samples, \n",
    "        key=kk, \n",
    "        epsilon=epsilon, \n",
    "        score_func=nn_score_func, \n",
    "        params=param, \n",
    "        init_scale=10, \n",
    "        starter_xs=starter_xs,\n",
    "    )\n",
    "    sampler_starter_xs_record.append(starter_xs)\n",
    "    chain_samples_record.append(chain_samples_annealed)\n",
    "    axes.flatten()[i].scatter(*final_states.T, label=f\"Noise {scale:.2f}\", color=\"black\", alpha=0.1)\n",
    "    axes.flatten()[i].scatter(*noised_up_data[:, -(i+1), :].T, label=\"Data\", alpha=0.1)\n",
    "    axes.flatten()[i].legend()\n",
    "    axes.flatten()[i].set_title(f\"Scale: {scale:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "\n",
    "I want to see if we are actually doing the thing that I though we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera \n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 8), ncols=2, sharex=True, sharey=True)\n",
    "cam = Camera(fig)\n",
    "\n",
    "\n",
    "for i, scale_chain_record in enumerate(chain_samples_record):\n",
    "    for step in range(0, n_samples, int(n_samples / 50)):\n",
    "        axes[0].scatter(*noised_up_data[:, -(i+1), :].T, color=\"black\", alpha=0.1)\n",
    "        axes[1].scatter(*scale_chain_record[:, step, :].T, color=\"blue\", alpha=0.1)\n",
    "        cam.snap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = cam.animate()\n",
    "animation.save(\"annealed_sampling_joint.mp4\", dpi=300, fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a062a7e1adbb829192b5a56a463e7bc3d2201a3b05feadd05a7a64f9805fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
