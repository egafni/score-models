{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Scales\n",
    "\n",
    "With a score function approximator, we have one small issue:\n",
    "in regions of low sample density,\n",
    "our estimate of the score function will be inaccurate,\n",
    "simply because we have few samples in those regimes.\n",
    "To get around this, we can:\n",
    "\n",
    "> perturb data points with noise \n",
    "> and train score-based models on the noisy data points instead.\n",
    "> When the noise magnitude is sufficiently large, \n",
    "> it can populate low data density regions \n",
    "> to improve the accuracy of estimated scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge tradeoff here, though:\n",
    "the larger the amount of perturbation,\n",
    "the greater the corruption of the input data.\n",
    "Let's see that in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, vmap\n",
    "import jax.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ecdf(data):\n",
    "    x, y = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "key = random.PRNGKey(45)\n",
    "k1, k2, k3 = random.split(key, 3)\n",
    "\n",
    "mix1 = random.normal(k1, shape=(1000,)) * 2 - 10\n",
    "mix2 = random.normal(k2, shape=(500,)) * 1 + 6\n",
    "\n",
    "data = np.concatenate([mix1, mix2]).flatten()\n",
    "plt.plot(*ecdf(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scales = np.array([1.0, 2.0, 5.0, 10.0])  # the first one doesn't have any changes!\n",
    "k1, k2 = random.split(k3)\n",
    "perturbations = random.normal(k1, shape=(len(data), len(noise_scales)))\n",
    "perturbations *= noise_scales\n",
    "data_perturbed = data.reshape(-1, 1) + perturbations\n",
    "\n",
    "for d, n in zip(data_perturbed.T, noise_scales):\n",
    "    plt.plot(*ecdf(d), label=f\"sigma={n}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be evident from the figure above\n",
    "that when we add more noise, the data look more and more like a single Gaussian\n",
    "and less like the original.\n",
    "Most crucially, in the regions of low density between the two mixture Gaussians,\n",
    "we have a much more nicely-defined PDF,\n",
    "and hence a better ability to compute the score function accurately,\n",
    "which we will be able to use when generating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that we can sample using score function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Want to make sure that we can sample from the blue curve\n",
    "using the procedure we showed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.sampler import langevin_dynamics\n",
    "from score_models.models import nn_model\n",
    "from score_models.losses import score_matching_loss\n",
    "\n",
    "from jaxopt import GradientDescent\n",
    "from functools import partial\n",
    "\n",
    "init_fun, nn_score_func = nn_model()\n",
    "k1, k2 = random.split(k3)\n",
    "_, params_init = init_fun(k1, input_shape=(None, 1))\n",
    "myloss = partial(score_matching_loss, score_func=nn_score_func)\n",
    "solver = GradientDescent(fun=myloss, maxiter=1200)\n",
    "result = solver.run(params_init, batch=data.reshape(-1, 1))\n",
    "initial_states, final_states, chain_samples_naive = langevin_dynamics(\n",
    "    n_chains=10000, \n",
    "    n_samples=2000, \n",
    "    key=key, \n",
    "    epsilon=5e-3, \n",
    "    score_func=nn_score_func, \n",
    "    params=result.params, \n",
    "    init_scale=10,\n",
    "    sample_shape=(None, 1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ecdf(final_states.flatten()), label=f\"score model langevin\")\n",
    "plt.plot(*ecdf(data), label=f\"sigma=1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, yes, we're able to!\n",
    "The weights are off,\n",
    "but at least we can _sample_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One score model per perturbation\n",
    "\n",
    "One key idea in Yang Song's blog post\n",
    "is that we can train score models for each of the noise levels\n",
    "and then use Langevin dynamics in an annealed fashion\n",
    "to progressively obtain better and better samples.\n",
    "In our example, we will have four models trained with a single loss function,\n",
    "which is the weighted sum of Fisher divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint loss\n",
    "from score_models.models import nn_model\n",
    "from functools import partial\n",
    "\n",
    "key = random.PRNGKey(44)\n",
    "\n",
    "# Four models\n",
    "init_fun, apply_fun = nn_model()\n",
    "keys = random.split(key, 4)\n",
    "key = random.split(keys[-1])[0]\n",
    "_, params_init = vmap(partial(init_fun, input_shape=(None, 1)))(keys)  # 4 sets of params now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import score_matching_loss\n",
    "from typing import Callable\n",
    "from jax import jit \n",
    "\n",
    "\n",
    "def multi_scale_loss(params: list, score_func: Callable, batch: np.ndarray, scales: np.ndarray):\n",
    "    \"\"\"Joint loss function.\n",
    "    \n",
    "    :param params: Should be a list of params for `score_func`, \n",
    "        should be of length equal to `scales`.\n",
    "    :param score_func: A function that estimates the score of a data point.\n",
    "        It is vmapped over `params`.\n",
    "    :param batch: A collection of data points of length equal to `scales`.\n",
    "        Should be of shape (n_observations, n_scales).\n",
    "    :param scales: Noise perturbation scale parameter.\n",
    "        Should be equal to the number of perturbations made.\n",
    "    \"\"\"\n",
    "    batch = batch.T  # shape: (n_scales, n_observations)\n",
    "    lossfunc = partial(score_matching_loss, score_func=score_func)\n",
    "    loss = vmap(lossfunc)(params, batch=batch)\n",
    "    return np.sum(loss) \n",
    "\n",
    "\n",
    "multi_scale_loss(params_init, nn_score_func, data_perturbed, noise_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxopt import GradientDescent\n",
    "from jax import jit \n",
    "\n",
    "joint_loss_func = jit(partial(multi_scale_loss, score_func=nn_score_func))\n",
    "solver = GradientDescent(joint_loss_func, maxiter=5000)\n",
    "result = solver.run(params_init, batch=data_perturbed, scales=noise_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result.params), len(params_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm sampling works after joint training\n",
    "\n",
    "Need to make sure that after joint training,\n",
    "samples from the first set of params approximate most closely the true data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map\n",
    "# Get out the params independently\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=4, sharex=True, sharey=True)\n",
    "for i, scale in enumerate(noise_scales):\n",
    "    param = tree_map(lambda x: x[i], result.params)\n",
    "    initial_states, final_states, chain_samples_joint = langevin_dynamics(\n",
    "        n_chains=2000, \n",
    "        n_samples=10000, \n",
    "        key=k1, \n",
    "        epsilon=5e-3, \n",
    "        score_func=nn_score_func, \n",
    "        params=param,\n",
    "        init_scale=3, \n",
    "        sample_shape=(None, 1)\n",
    "    )\n",
    "    final_states = np.clip(final_states, -30, 30)\n",
    "    axes[i].plot(*ecdf(final_states.flatten()), label=f\"Perturbation: {scale}\", color=\"black\")\n",
    "    axes[i].plot(*ecdf(data), label=\"Data\")\n",
    "    axes[i].legend()\n",
    "    axes[i].set_title(f\"Perturbation: {scale}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not bad.\n",
    "We see that the samples drawn from Perturbation 1.0 (i.e. no perturbation)\n",
    "match the closest to the samples drawn from perturbation 10.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealed Langevin Dynamics\n",
    "\n",
    "We now implement annealed Langevin dynamics,\n",
    "where we sequentially sample from the data distributions at each noise level,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I probably have an issue here with annealed sampling...\n",
    "from jax.tree_util import tree_map\n",
    "from score_models.sampler import langevin_dynamics\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 4), ncols=4, sharex=True)\n",
    "n_chains = 40000\n",
    "k1, k2 = random.split(key)\n",
    "starter_xs = random.normal(k1, shape=(n_chains, 1))\n",
    "sampler_starter_xs_record = [starter_xs]\n",
    "chain_samples_record = []\n",
    "epsilon = 5e-3\n",
    "# We start first by sampling from the \n",
    "for i, scale in enumerate(noise_scales[::-1]):\n",
    "    k1, k2 = random.split(k2)\n",
    "    param = tree_map(lambda x: x[-i], result.params)\n",
    "    _, starter_xs, chain_samples_annealed = langevin_dynamics(\n",
    "        n_chains=n_chains, \n",
    "        n_samples=1000, \n",
    "        key=k1, \n",
    "        epsilon=epsilon, \n",
    "        score_func=nn_score_func, \n",
    "        params=param, \n",
    "        init_scale=10, \n",
    "        starter_xs=starter_xs,\n",
    "    )\n",
    "    sampler_starter_xs_record.append(starter_xs)\n",
    "    chain_samples_record.append(chain_samples_annealed)\n",
    "    \n",
    "    dist = wasserstein_distance(data.flatten(), starter_xs.flatten())\n",
    "\n",
    "    axes[i].plot(*ecdf(starter_xs.flatten()), label=f\"Noise {scale}\", color=\"black\")\n",
    "    axes[i].plot(*ecdf(data), label=\"Data\")\n",
    "    axes[i].legend()\n",
    "    axes[i].set_title(f\"Distance: {dist:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a tighter and tighter match to the original data distribution. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a062a7e1adbb829192b5a56a463e7bc3d2201a3b05feadd05a7a64f9805fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
