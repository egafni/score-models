[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Pedagogical Introduction to Score Models",
    "section": "",
    "text": "One of the coolest new developments in generative modelling is the use of score models to generate samples that look like an existing collection of samples. Pioneering this work is Yang Song, who did this work while a PhD student at Stanford University. The way I first uncovered his work is through Twitter, where I was brought to his excellently written blog post on the topic.\nIn this collection of notebooks, I would like to explore the fundamental ideas that underlie his work. Along the way, we will work towards a pedagogical implementation of score models as generative models. By the end of this journey, we should have a much better understanding of score models and their core concepts, and should also have a framework for writing the code necessary to implement score models in JAX and Python."
  },
  {
    "objectID": "notebooks/01-score-function.html#definitions",
    "href": "notebooks/01-score-function.html#definitions",
    "title": "2  Score Functions",
    "section": "2.1 Definitions",
    "text": "2.1 Definitions\nWhat’s a score function? The score function is defined as follows:\n\nThe score function is the gradient of the log of the probability density function of a probability distribution with respect to the distribution’s support.\n\nThere’s a lot to unpack in there, so let’s dissect the anatomy of this definition bit by bit.\n\n2.1.1 Probability Distributions\nProbability distributions are super cool objects in stats1. Distributions can be configured through their parameters; for example, by setting the values \\(\\mu\\) and \\(\\sigma\\) of a Gaussian respectively. We can use probability distributions to generate data, and we can use them to evaluate the likelihood of observed data. The latter point is done by using a probability distribution’s probability density function2.\n\n\n2.1.2 \\(P(x)\\) a.k.a. Probability Density Function\nA distribution’s probability density function (PDF) describes the propensity of a probability distribution to generate draws of a particular value. As mentioned above, we primarily use the PDF to evaluate the likelihood of the observing data, given the distribution’s configuration. If you need an anchoring example, think of the venerable Gaussian probability density function in Figure 2.1.\n\n\n\n\n\nFigure 2.1: \\(P(x)\\) (likelihood, PDF), \\(log P(x)\\) (log likelihood, logp), and \\(dlogP(x)\\) (score) of a Gaussian.\n\n\n\n\nEvery distribution has a support, which is the range of values for which the probability distribution is defined. The Gaussian has support in the range \\((-\\infty, \\infty)\\), while positive-only distributions (such as the Exponential) have support in the range \\((0, \\infty)\\).\n\n\n2.1.3 \\(log P(x)\\) a.k.a. Log PDF\nBecause the PDF is nothing more than a math function, we can take its logarithm! In computational statistics, taking the log is usually done for pragmatic purposes, as we usually end up with underflow issues otherwise. For the standard Gaussian above, its log PDF looks like what we see in Figure 2.1.\nWe often call the log PDF logp for short, and in the probabilistic programming language PyMC, logp is the name of the class method use for calculating the log likelihood of data under the distribution.\n\n\n2.1.4 \\(dlogP(x)\\) a.k.a. Score Function\nFinally, we get to the score. As it turns out, because the logp function is differentiable, we can take its derivative easily (using JAX, for example). The derivative of the logp function is called the score function. The score of a distribution is the gradient of the logp function w.r.t. the support. You can visualize what it is like in Figure 2.1.\nIn JAX, obtaining the score function is relatively easy. We simply need to use JAX’s grad function to obtain the transformed logp function.\n\nfrom jax import grad \ngaussian_score = grad(norm.logpdf)\n\nSince we’re using a Gaussian as our anchoring example, let’s examine some properties of the score function. From visual inspection above, we know that at the top of the Gaussian, the gradient should be zero, and can verify as much.\n\ngaussian_score(0.0)\n\nArray(-0., dtype=float32, weak_type=True)\n\n\nAt the tails, the gradient should be of higher magnitude than at the mean.\n\ngaussian_score(-3.0), gaussian_score(3.0)\n\n(Array(3., dtype=float32, weak_type=True),\n Array(-3., dtype=float32, weak_type=True))"
  },
  {
    "objectID": "notebooks/01-score-function.html#estimating-the-score-function",
    "href": "notebooks/01-score-function.html#estimating-the-score-function",
    "title": "2  Score Functions",
    "section": "2.2 Estimating the score function",
    "text": "2.2 Estimating the score function\nIn 2005, Aapo Hyvärinen published a paper in the Journal of Machine Learning Research that details how to estimate the score function in the absence of knowledge of the true data generating distribution (Hyvärinen 2005). When I first heard of the idea, I thought it was crazy – crazy cool that we could even do this!\nSo how do we use data to estimate the score of that data without knowing the true probability density? One key equation in the paper is equation #4. This equation details how we can use an arbitrary function, \\(\\psi(x, \\theta)\\), to approximate the score function, and the loss function needed to train the parameters of the function \\(\\theta\\) to approximate the score function. I’ve replicated the equation below, alongside a bullet-point explanation of what each of the terms are:\n\\[J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i(x(t); \\theta)^2 ] + \\text{const}\\]\nHere:\n\n\\(J(\\theta)\\) is the loss function that we wish to minimize w.r.t. the parameters \\(\\theta\\)\n\\(\\theta\\) are the parameters of the function \\(\\psi_i\\)\n\\(\\psi_i(x(t); \\theta)\\) is the multidimensional score function estimator for \\(x\\). \\(\\psi_i\\) has parameters \\(\\theta\\).\n\nThe subscript \\(i\\) is a dimensional indexer. If \\(x\\) is 2-dimensional, then \\(i=2\\).\n\n\\(x(t)\\) are the i.i.d. samples from the unknown data-generating distribution.\n\\(\\delta_i\\) refers to the partial derivative w.r.t. dimension \\(i\\) in \\(x\\).\n\\(\\text{const}\\) is a constant term that effectively can be ignored.\n\nLet’s explore the idea in a bit more detail. What we’re ultimately going to do here is use a simple feed-forward neural network as the score function estimator \\(\\psi(x(t), \\theta)\\). Let’s start first by generating the kind of data that’s needed for score function estimation to work.\n\n\nCode\nfrom jax import random\n\nkey = random.PRNGKey(44)\n\ntrue_mu = 3.0\ntrue_sigma = 1.0\ndata = random.normal(key, shape=(1000, 1)) * true_sigma + true_mu\ndata[0:5]  # showing just the first 10 samples drawn\n\n\nArray([[0.8219464],\n       [3.8602278],\n       [1.4089172],\n       [3.4423368],\n       [3.2420166]], dtype=float32)\n\n\nBefore we go on, we should also verify that the \\(\\mu\\) and \\(\\sigma\\) of the data are as close to the ground truth as possible.\n\ndata.mean(), data.std()\n\n(Array(2.9986677, dtype=float32), Array(1.0197434, dtype=float32))\n\n\n\n2.2.1 Baseline: evaluate score under known PDF\nNow, let’s try to evaluate the score function directly. The purpose here is to establish a baseline model to compare against and to set up the patterns for training a neural network model. In anticipation of writing neural network models later, I have opted to write our models, neural network or otherwise, in the style of Equinox (Kidger and Garcia 2021), which provides a way to associate model parameters with a callable object directly while maintaining compatibility with the rest of the JAX ecosystem.\n\nfrom score_models.models.gaussian import GaussianModel\nfrom inspect import getsource\n\nprint(getsource(GaussianModel))\n\nclass GaussianModel(eqx.Module):\n    \"\"\"Univariate Gaussian score function.\"\"\"\n\n    mu: np.array = np.array(0.0)\n    log_sigma: np.array = np.array(0.0)\n\n    @eqx.filter_jit\n    def __call__(self, x):\n        \"\"\"Forward pass.\n\n        :param x: Data. Should be of shape (1, :),\n            where `1` is in the batch dimension slot.\n            as the model is intended to be vmapped over batches of data.\n        :returns: Score of a Gaussian conditioned on a `mu` and `log_sigma`.\n        \"\"\"\n        gaussian_score_func = jacfwd(norm.logpdf)\n        return gaussian_score_func(x, loc=self.mu, scale=np.exp(self.log_sigma))\n\n\n\nAbove, instead of grad, we are using jacfwd, which gives us the Jacobian of norm.logpdf. The Jacobian is a generalization of the first derivative, extended to matrix inputs. To test that we have the implementation done correct, let’s ensure that we can evaluate the GaussianModel score function at a few special points.\n\ngaussian_model = GaussianModel()\n(\n    gaussian_model(-3.0),\n    gaussian_model(0.0),\n    gaussian_model(3.0),\n)\n\n(Array(3., dtype=float32, weak_type=True),\n Array(-0., dtype=float32, weak_type=True),\n Array(-3., dtype=float32, weak_type=True))\n\n\nLet’s also ensure that we can evalute the score function at each data point. We will use the vmap function to explicitly map score_func across all data points.\n\ndata_score = vmap(gaussian_model)(data).squeeze()\ndata_score[0:5]\n\nArray([-0.8219464, -3.8602278, -1.4089172, -3.4423368, -3.2420166],      dtype=float32)\n\n\n\n\n2.2.2 Train baseline score function model\nHere, we’ve instantiated the Gaussian with default parameters (\\(\\mu=0\\) and \\(\\sigma=1\\)), but those aren’t the true configuration of the underlying data-generating process. Hence, our score calculated scores are going to be way off, as is visible in Figure 2.2.\n\n\n\n\n\nFigure 2.2: Comparison of score function evaluated under the true vs. incorrect data-generating distribution parameters\n\n\n\n\nSince the model is wrong, we’re going to see if we can make it right. One generic way to train models is to use gradient descent; that’s what we’ll use here. For us, we’ll be using optax alongside a fitting routine that I have implemented before.\nFinally, we’ve reached the point where we can implement the score function loss in JAX! Let’s see it below, with the earlier equation from above copied down here for convenience.\n\\[J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i(x(t); \\theta)^2 ] + \\text{const}\\]\nWe can now train the model.\n\nfrom score_models.training import fit\nfrom score_models.losses import score_matching_loss\nimport optax\n\noptimizer = optax.adam(learning_rate=5e-3)\nupdated_model, loss_history = fit(\n    gaussian_model, \n    data, \n    score_matching_loss, \n    optimizer, \n    steps=2_000, \n    progress_bar=False\n)\n\nLet’s take a peek at the loss curve to make sure our model is fitting.\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n\n\nplt.sca(axes[0])\nplt.plot(loss_history)\nplt.xlabel(\"Training Iteration\")\nplt.ylabel(\"Score Matching Loss\")\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\nplt.sca(axes[1])\nupdated_model_scores = vmap(updated_model)(data)\nplt.scatter(updated_model_scores.squeeze(), true_model_scores.squeeze())\nplt.xlabel(\"Estimated Scores\")\nplt.ylabel(\"True Model\")\nplt.title(\"True vs. Estimated Score\")\nsns.despine()\n\n\n\n\n\nFigure 2.3: Loss curve for Gaussian score model.\n\n\n\n\nIt’s reassuring to see the loss decrease and the estimated scores match up with the predicted scores.\nAnd here are the mean and standard deviation of the data vs. the model estimates.\n\nnp.mean(data), np.std(data), updated_model.mu, np.exp(updated_model.log_sigma)\n\n(Array(2.9986677, dtype=float32),\n Array(1.0197434, dtype=float32),\n Array(2.9963465, dtype=float32),\n Array(1.0235366, dtype=float32))"
  },
  {
    "objectID": "notebooks/01-score-function.html#approximate-score-functions",
    "href": "notebooks/01-score-function.html#approximate-score-functions",
    "title": "2  Score Functions",
    "section": "2.3 Approximate Score Functions",
    "text": "2.3 Approximate Score Functions\nNow, the entire premise of Hyvärinen’s paper is that we need not know the original form of the PDF and we’d still be able to estimate the score at each data point. Since the score function is smooth and differentiable, we can reach for the venerable neural network, a.k.a. the universal function approximator, as our estimation model for the score of our data.\n\n2.3.1 Neural Network Approximator\nHere, we will set up a single feed-forward neural network model with 1 hidden layer of width 1024 and a ReLU activation function. Here is the source of my wrapper implementation around Equinox’s MLP.\n\nfrom score_models.models.feedforward import FeedForwardModel1D\n\nprint(getsource(FeedForwardModel1D))\n\nclass FeedForwardModel1D(eqx.Module):\n    \"\"\"Feed-forward NN model.\"\"\"\n\n    mlp: eqx.Module\n\n    def __init__(\n        self,\n        in_size=1,\n        out_size=1,\n        width_size=4096,\n        depth=1,\n        activation=nn.relu,\n        key=random.PRNGKey(45),\n    ):\n        self.mlp = eqx.nn.MLP(\n            in_size=in_size,\n            out_size=out_size,\n            width_size=width_size,\n            depth=depth,\n            activation=activation,\n            key=key,\n        )\n\n    @eqx.filter_jit\n    def __call__(self, x):\n        \"\"\"Forward pass.\n\n        :param x: Data. Should be of shape (1, :),\n            as the model is intended to be vmapped over batches of data.\n        :returns: Estimated score of a Gaussian.\n        \"\"\"\n        return self.mlp(x)\n\n\n\n\n\n2.3.2 Training Objective/Loss Function\nFor our loss function, I know from previous experience with these models that we could get weights that explode to really large magnitudes. To control this, I have chained in an L2 regularization on the weights on the loss function.\n\nfrom jax import nn \nfrom score_models import losses\n\nregularized_loss = losses.chain(\n    losses.l2_norm, \n    losses.score_matching_loss,\n)\n\nNow, we can train the model.\n\nffmodel = FeedForwardModel1D(depth=1, width_size=1024, activation=nn.relu)\noptimizer = optax.chain(\n    optax.clip(0.01),\n    optax.sgd(learning_rate=5e-3),\n)\nupdated_model, history = fit(\n    ffmodel,\n    data,\n    regularized_loss,\n    optimizer,\n    steps=2_000,\n    progress_bar=False,\n)\n\n\n\n2.3.3 Approximator Performance\nNow, let’s visualize the training loss and how the model compares to ground truth.\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n\nplt.sca(axes[0])\nplt.plot(loss_history)\nplt.xlabel(\"Training Iteration\")\nplt.ylabel(\"Loss Value\")\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\nplt.sca(axes[1])\nupdated_model_scores = vmap(updated_model)(data).squeeze()\nplt.scatter(data.squeeze(), true_model_scores, label=\"True Model Scores\")\nplt.scatter(data.squeeze(), updated_model_scores, label=\"Feed Forward Estimate\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.title(\"True vs. Estimated Score\")\nplt.legend()\nsns.despine()\n\n\n\n\n\nFigure 2.4: Loss curve and performance plot for Neural Network score model.\n\n\n\n\nThis isn’t bad at all! We’re off by a bit, but keep in mind that we only had data on hand and didn’t know what the exact data-generating density is. We should expect to be a bit off."
  },
  {
    "objectID": "notebooks/01-score-function.html#mixture-distributions",
    "href": "notebooks/01-score-function.html#mixture-distributions",
    "title": "2  Score Functions",
    "section": "2.4 Mixture Distributions",
    "text": "2.4 Mixture Distributions\nWhile data drawn from a Gaussian is nice and ideal, you won’t really be able to tell if your data came from a Gaussian3. In addition, most data would have the characteristics of being generated from a mixture distribution. In other words, mixture distributions are what our data will look the most like. Let’s make sure our approximate score function can approximate the mixture distribution scores as accurately as possible, at least in 1 dimension.\n\n2.4.1 Mixture Gaussian Model\nWe have a MixtureGaussian model that implements the score of a mixture Gaussian distribution. Its source is below:\n\nfrom score_models.models.gaussian import MixtureGaussian\n\nprint(getsource(MixtureGaussian))\n\nclass MixtureGaussian(eqx.Module):\n    \"\"\"Mixture Gaussian score function.\"\"\"\n\n    mus: np.array\n    log_sigmas: np.array\n    ws: np.array\n\n    def __init__(self, mus, log_sigmas, ws):\n        self.mus = mus\n        self.log_sigmas = log_sigmas\n        self.ws = ws\n\n        # Check that mus, log_sigmas, and ws are of the same length.\n        lengths = set(map(len, [mus, log_sigmas, ws]))\n        if len(lengths) != 1:\n            raise ValueError(\n                \"`mus`, `log_sigmas` and `ws` must all be of the same length!\"\n            )\n\n    @eqx.filter_jit\n    def __call__(self, x):\n        \"\"\"Forward pass.\n\n        :param x: Data. Should be of shape (1, :),\n            where `1` is in the batch dimension slot.\n            as the model is intended to be vmapped over batches of data.\n        :returns: Score of a Gaussian conditioned on a `mu` and `log_sigma`.\n        \"\"\"\n        return partial(\n            dmixture_logpdf,\n            mus=self.mus,\n            sigmas=np.exp(self.log_sigmas),\n            ws=self.ws,\n        )(x)\n\n\n\n\n\n2.4.2 Mixture Gaussian Score Function\nLet’s use the model to plot Gaussian data and the true Gaussian mixture score.\n\n\nCode\nimport seaborn as sns \nimport numpy as onp \n\nx = np.linspace(-10, 10, 200)\nmus = np.array([-3, 3])\nsigmas = np.array([1, 1])\nws = np.array([0.1, 0.9])\n\nmgmodel = MixtureGaussian(mus, np.log(sigmas), ws)\nmixture_logpdf_grads = vmap(mgmodel)(x)\n\n\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n\nplt.sca(axes[0])\nk1, k2 = random.split(random.PRNGKey(55))\ndraws = 1000\nmix1 = random.normal(k1, shape=(1000,)) * 1 - 3\nmix2 = random.normal(k2, shape=(9000,)) * 1 + 3\ndata = np.concatenate([mix1, mix2]).reshape(-1, 1)\nplt.hist(onp.array(data), bins=100)\nplt.title(\"Mixture Gaussian Histogram\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Count\")\nsns.despine()\n\nplt.sca(axes[1])\nplt.plot(x, mixture_logpdf_grads)\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.title(\"Mixture Gaussian Score\")\nsns.despine()\n\n\n\n\n\nFigure 2.5: A two-component mixture Gaussian and its score function\n\n\n\n\n\n\n2.4.3 Train neural network approximator\nThen, we’re going to use the feed forward neural network model from before to try to fit the mixture Gaussian data above.\n\noptimizer = optax.chain(\n    optax.clip(0.01),\n    optax.sgd(learning_rate=5e-3),\n)\nupdated_model, history = fit(\n    ffmodel,\n    data,\n    losses.score_matching_loss,\n    optimizer,\n    steps=1_000,\n    progress_bar=False,\n)\n\nLet’s see how the loss looks like:\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n\nplt.sca(axes[0])\nplt.plot(loss_history)\nplt.xlabel(\"Training Iteration\")\nplt.ylabel(\"Score Matching Loss\")\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\nplt.sca(axes[1])\nmixture_est_scores = vmap(updated_model)(data)\nplt.plot(x, mixture_logpdf_grads, label=\"Ground Truth\")\nplt.scatter(data, mixture_est_scores, label=\"Estimated\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.title(\"True vs. Estimated Score\")\nplt.legend()\nsns.despine()\n\n\n\n\n\nFigure 2.6: Loss curve for neural network score model approximator for 2-component mixture Gaussian.\n\n\n\n\nNot bad! It’s clear to me that we can approxiate the score function of a 2-component mixture Gaussian here."
  },
  {
    "objectID": "notebooks/01-score-function.html#component-mixture",
    "href": "notebooks/01-score-function.html#component-mixture",
    "title": "2  Score Functions",
    "section": "2.5 3-Component Mixture",
    "text": "2.5 3-Component Mixture\nWe’re now going to see whether we can approximate a 3-component mixture. This will be the example that rounds out this chapter. Firstly, let’s see draws from a 3-component mixture model and the score function of this mixture distribution.\n\n\nCode\nmus = np.array([-7, -2, 3])\nsigmas = np.ones(3)\nws = np.array([0.1, 0.4, 0.5])\n\n\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2, sharex=True)\nplt.sca(axes[0])\nk1, k2, k3 = random.split(random.PRNGKey(91), 3)\ndraws = 1000\nmix1 = random.normal(k1, shape=(100,)) * 1 - 7\nmix2 = random.normal(k2, shape=(400,)) * 1 - 2\nmix3 = random.normal(k3, shape=(500,)) * 1 + 3\ndata = np.concatenate([mix1, mix2, mix3]).reshape(-1, 1)\nplt.hist(onp.array(data), bins=100)\nplt.xlabel(\"Support\")\nplt.ylabel(\"Count\")\nplt.title(\"Mixture Gaussian Histogram\")\nsns.despine()\n\nplt.sca(axes[1])\nx = np.linspace(-11, 6, 1000)\nthree_comp_gaussian = MixtureGaussian(mus, np.log(sigmas), ws)\n\nmixture_logpdf_grads = vmap(three_comp_gaussian)(x)\nplt.plot(x, mixture_logpdf_grads)\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.title(\"Mixture Gaussian Score\")\nsns.despine()\n\n\n\n\n\nFigure 2.7: 3-component mixture Gaussian score function.\n\n\n\n\nNow, let’s train a 3-component Gaussian model and check its performance. This again serves as our baseline.\n\n# Sanity check that this works with a MixtureGaussianModel\nthree_comp_gaussian_est = MixtureGaussian(mus, np.log(sigmas), np.ones(3) / 3)\noptimizer = optax.chain(\n    optax.clip(0.001),\n    optax.adam(learning_rate=5e-3),\n)\nupdated_model, history = fit(\n    three_comp_gaussian_est, data, losses.score_matching_loss, optimizer, steps=100, progress_bar=False\n)\n\n\n\nCode\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\nplt.sca(axes[0])\nplt.plot(history)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\n\nplt.sca(axes[1])\nmixture_est_scores = vmap(updated_model)(data)\nplt.plot(x, mixture_logpdf_grads, label=\"Ground Truth\")\nplt.scatter(data, mixture_est_scores, label=\"Estimated Scores\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.title(\"True vs. Baseline Score\")\nsns.despine()\n\n\n\n\n\nFigure 2.8: Training performance of a 3-component Gaussian model trained on the sampled data.\n\n\n\n\nNow, we’re going to train a neural network model.\n\nfrom functools import partial\n\n\n# NOTE: I needed to tweak this neural network's\n# activation function, architecture, and number of training steps quite a bit.\n# Recording here the thing that trains most stably:\n# - 2000 steps\n# - depth = 2, width_size = 512, activation = nn.softplus\n# - optax.clip(0.001), optax.adam(learning_rate = 5e-3)\n# - lossses.score_matching_loss, don't do regularized.\n\nffmodel = FeedForwardModel1D(depth=2, width_size=512, activation=nn.softplus)\noptimizer = optax.chain(\n    optax.clip(0.0001),\n    optax.adam(learning_rate=5e-3),\n)\n\nupdated_model, history = fit(\n    ffmodel,\n    data,\n    losses.score_matching_loss,\n    optimizer,\n    steps=5_000,\n    progress_bar=True,\n)\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nplt.sca(axes[0])\nplt.plot(history)\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\nplt.sca(axes[1])\nmixture_est_scores = vmap(updated_model)(data)\nplt.plot(x, mixture_logpdf_grads, label=\"True\")\nplt.scatter(data, mixture_est_scores, label=\"Estimated\")\nplt.title(\"True vs. Estimated Score\")\nsns.despine()\n\n\n\n\n\nFigure 2.9: Training performance of feed forward neural network on 3-component Gaussian mixture.\n\n\n\n\nNot bad, this actually works! Although in this case we do know the true score function, we are actually trying to estimate it in the presence of draws from the data-generating distribution while pretending to not know the true data-generating distribution. What I’ve tried to show here is that a neural network model can approximate the true score function, as shown in Figure 2.9 and Figure 2.6."
  },
  {
    "objectID": "notebooks/01-score-function.html#up-next",
    "href": "notebooks/01-score-function.html#up-next",
    "title": "2  Score Functions",
    "section": "2.6 Up Next",
    "text": "2.6 Up Next\nComing up next is how we sample from a distribution when knowing only the score function, true or estimated."
  },
  {
    "objectID": "notebooks/01-score-function.html#references",
    "href": "notebooks/01-score-function.html#references",
    "title": "2  Score Functions",
    "section": "2.7 References",
    "text": "2.7 References\n\n\n\n\nHyvärinen, Aapo. 2005. “Estimation of Non-Normalized Statistical Models by Score Matching.” Journal of Machine Learning Research 6 (24): 695–709. http://jmlr.org/papers/v6/hyvarinen05a.html.\n\n\nKidger, Patrick, and Cristian Garcia. 2021. “Equinox: Neural Networks in JAX via Callable PyTrees and Filtered Transformations.” Differentiable Programming Workshop at Neural Information Processing Systems 2021."
  },
  {
    "objectID": "notebooks/01-score-function.html#footnotes",
    "href": "notebooks/01-score-function.html#footnotes",
    "title": "2  Score Functions",
    "section": "",
    "text": "I’ve explored the anatomy of a probability distribution in my essay on Bayesian and computational statistics, and would recommend looking at it for a refresher.↩︎\nOr the probability mass function, for discrete distributions, but we’re going to stick with continuous distributions for this essay.↩︎\nAllen Downey has an excellent blog post on this matter.↩︎"
  },
  {
    "objectID": "notebooks/02-langevin-dynamics.html#sampling-from-a-density-without-knowing-the-density",
    "href": "notebooks/02-langevin-dynamics.html#sampling-from-a-density-without-knowing-the-density",
    "title": "3  Langevin Dynamics",
    "section": "3.1 Sampling from a density without knowing the density",
    "text": "3.1 Sampling from a density without knowing the density\nAs mentioned in the first chapter, one of the key motivations in using score models is to generate new data samples from existing samples. In the case of data such as images, audio, text, and other complicated modalities, the data generating distribution can’t be written down in some analytical form. In other words, complex data (images, audio, text, etc.) come from an unknown density. So how do we draw samples from that distribution that are similar to existing samples without having access to the actual density?\nThat situation is exactly where having an estimator of the score function is important! By estimating the score function using existing data, we can use score function approximator to guide us to another set of coordinates in the input space, thus yielding a new sample drawn from the data-generating density.\nSampling implies not simply following gradients naïvely. In other words, we’re not merely interested in following the gradients to another high likelihood position. Rather, sampling implies the use of stochasticity. One sampling strategy that provides us with gradients and stochasticity is called “Langevin dynamics”. Let’s explore what it is over here.\n\n3.1.1 Langevin dynamics, the algorithm\nAccording to Yang Song’s blog,\n\nLangevin dynamics provides an MCMC procedure to sample from a distribution \\(p(x)\\) using only its score function \\(\\nabla_x \\log p(x)\\). Specifically, it initializes the chain from an arbitrary prior distribution \\(x_0 \\sim \\pi(x)\\), and then iterates the following:\n\\[x_{i+1} \\leftarrow x_i + \\epsilon \\nabla_x \\log p(x) + \\sqrt{2 \\epsilon} z_i\\]\n\nwhere \\(i = 0, 1, ... K\\) and \\(z_i \\sim \\text{Normal}(0, I)\\) is a multivariate Gaussian\nLet’s dissect each term in the equation above.\n\n\\(x_i, x_{i+1}, ...\\) refer to the draws that are sampled out of the procedure at each iteration \\(i\\).\n\\(\\nabla_x \\log p(x)\\) is the gradient of the logp of the density w.r.t. \\(x\\). This is exactly the score function that we’re trying to approximate with our models. This term gives us a step in the direction of the gradient.\n\\(\\sqrt{2 \\epsilon}z_i\\) is a term that injects noise into the procedure.\n\\(\\epsilon\\) is a scaling factor, akin to a hyperparameter, that lets us control the magnitude of the step in the gradient direction.\n\nAs you probably can see, we basically start at some point \\(x_i\\) in the input space \\(x\\), use the score function to move in a direction, but done with the injection of noise into the procedure to make it a stochastic procedure. As such, the new value \\(x_{i+1}\\) that we draw will be a value from the distribution \\(P(x)\\), but biased towards higher estimated densities by nature of following the gradient.\n\n\n3.1.2 Langevin dynamics, in Python\nLet’s see how that one Langevin dynamics step might be translated into Python:\nfrom jax import random, numpy as np\n\ndef langevin_dynamics_step(prev_x, score_func, epsilon, key):\n    \"\"\"One step of Langevin dynamics sampling.\"\"\"\n    draw = random.normal(key)\n    new_x = prev_x + epsilon * score_func(prev_x) + np.sqrt(2 * epsilon) * draw\n    return new_x"
  },
  {
    "objectID": "notebooks/02-langevin-dynamics.html#a-worked-example-with-1d-univarite-gaussians",
    "href": "notebooks/02-langevin-dynamics.html#a-worked-example-with-1d-univarite-gaussians",
    "title": "3  Langevin Dynamics",
    "section": "3.2 A worked example with 1D univarite Gaussians",
    "text": "3.2 A worked example with 1D univarite Gaussians\nLet’s walk through a worked example that uses 1D Normal distributions. We will start with a mixture Gaussian distribution that has two components, estimate the score function of the mixture Gaussian using a neural network, and then use the score function to do sampling of new draws from the Gaussian.\n\n\n\n\n\nFigure 3.1: Empirical cumulative distribution function (ECDF) and histogram of a 2-component mixture Gaussian data.\n\n\n\n\n\n3.2.1 Train a score function model\nAs with before, we will train an approximate score function on this mixture Gaussian data. The model architecture will be a simple feed-forward neural network.\n\nfrom score_models.training import fit\nfrom score_models.models import FeedForwardModel1D\nfrom score_models.losses import score_matching_loss\nimport optax\n\nffmodel = FeedForwardModel1D()\n\noptimizer = optax.adam(learning_rate=5e-3)\nupdated_model, loss_history = fit(\n    ffmodel, \n    data, \n    score_matching_loss, \n    optimizer, \n    steps=2_000, \n    progress_bar=False\n)\n\nLet us now diagnose whether we converged.\n\n\nCode\nfrom jax import vmap\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n\n\nplt.sca(axes[0])\nplt.plot(loss_history)\nplt.xlabel(\"Training Iteration\")\nplt.ylabel(\"Score Matching Loss\")\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\nplt.sca(axes[1])\nupdated_model_scores = vmap(updated_model)(data)\nplt.scatter(data.squeeze(), updated_model_scores.squeeze())\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.title(\"Estimated Scores\")\nsns.despine()\n\n\n\n\n\nFigure 3.2: Loss curve for Gaussian score model.\n\n\n\n\nFrom what we know about how the score function of a 2-component mixture should look like, It is safe to say that we have converged and can use the trained model. One thing should be noted here: we have explicitly avoided doing train/val/test splits here, but doing so is recommended! Just as with any other loss function for predicting classes or real numbers, we would use splitting here to determine when to stop training.\n\n\n3.2.2 Sample using the score function\nWe are now going to attempt to use the neural network score approximator in a Langevin dynamics MCMC sampler. Langevin dynamics, being an iterative MCMC sampler, needs the use of a for-loop with carryover construct. I have taken advantage of jax.lax.scan for fast, compiled looping with carryover. In addition to that, because the operation requires parameterization of a function, Equinox is another natural choice for its implementation.\n\n\nCode\nfrom score_models.sampler import LangevinDynamicsChain\nfrom inspect import getsource\n\nprint(getsource(LangevinDynamicsChain))\n\n\nclass LangevinDynamicsChain(eqx.Module):\n    \"\"\"Langevin dynamics chain.\"\"\"\n\n    gradient_func: eqx.Module\n    n_samples: int = 1000\n    epsilon: float = 5e-3\n\n    @eqx.filter_jit\n    def __call__(self, x, key: random.PRNGKey):\n        \"\"\"Callable implementation for sampling.\n\n        :param x: Data of shape (batch, :).\n        :param key: PRNGKey for random draws.\n        :returns: A tuple of final draw and historical draws.\"\"\"\n\n        def langevin_step(prev_x, key):\n            \"\"\"Scannable langevin dynamics step.\n\n            :param prev_x: Previous value of x in langevin dynamics step.\n            :param key: PRNGKey for random draws.\n            :returns: A tuple of new x and previous x.\n            \"\"\"\n            draw = random.normal(key, shape=x.shape)\n            new_x = (\n                prev_x\n                + self.epsilon * vmap(self.gradient_func)(prev_x)\n                + np.sqrt(2 * self.epsilon) * draw\n            )\n            return new_x, prev_x\n\n        keys = random.split(key, self.n_samples)\n        final_xs, xs = lax.scan(langevin_step, init=x, xs=keys)\n        return final_xs, np.vstack(xs)\n\n\n\n\n\n3.2.3 Sample one chain\nLet’s run one chain of the Langevin dynamics sampler to see what the samples from one chain look like. For comparison, we will show what the sampler draws look like when we have an untrained model vs. a trained model, and so we will have two samplers instantiated as well.\n\ntrained_model_sampler = LangevinDynamicsChain(gradient_func=updated_model, epsilon=5e-1)\nkey = random.PRNGKey(55)\nfinal, trained_samples = trained_model_sampler(np.array([[2.0]]), key)\n\nuntrained_model_sampler = LangevinDynamicsChain(gradient_func=ffmodel, epsilon=5e-1)\nfinal, untrained_samples = untrained_model_sampler(np.array([[2.0]]), key)\n\nNow that the Langevin dynamics samplers have been instantiated and run for one chain, let’s see what our “draws” look like.\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 2.5), ncols=3, sharex=True)\n\nplt.sca(axes[0])\nplt.hist(onp.array(untrained_samples), bins=100)\nplt.title(\"(a) Untrained\")\nplt.xlabel(\"Support\")\n\nplt.sca(axes[1])\nplt.hist(onp.array(trained_samples), bins=100)\nplt.title(\"(b) Trained\")\nplt.xlabel(\"Support\")\n\nplt.sca(axes[2])\nplt.hist(onp.array(data), bins=100)\nplt.title(\"(c) Original data\")\nplt.xlabel(\"Support\")\n\nsns.despine()\n\n\n\n\n\nFigure 3.3: Draws from Langevin dynamics sampler. Samples from a sampler with an (a) untrained neural network score function and (b) trained neural network compared to (c) original sampled data.\n\n\n\n\nThat looks amazing! It looks abundantly clear to me that with one chain, we can draw new samples from our mixture distribution without needing to know the mixture distribution parameters! There isn’t perfect correspondence, but for the purposes of drawing new samples that look like existing ones, an approximate model appears to be good enough.\n\n\n3.2.4 Multi-Chain Sampling\nWe are now going to attempt multi-chain sampling! Let us instantiate 1,000 starter points drawn randomly from a Gaussian and then run the sampler for 200 steps. Note here that by designing our single chain sampler to be called on a single starter point (of the right shape) and a pseudorandom number generator key, we can vmap the sampling routine over multiple starter points and keys rather trivially.\n\nkey = random.PRNGKey(49)\nn_particles = 10_000\nstarter_points = random.normal(key, shape=(n_particles, 1, 1)) * 5\n\nstarter_keys = random.split(key, n_particles)\n\ntrained_model_sampler = LangevinDynamicsChain(\n    gradient_func=updated_model, n_samples=100, epsilon=5e-1\n)\nfinal, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 2.5), nrows=1, ncols=3, sharex=True)\n\nplt.sca(axes[0])\nplt.xlabel(\"Support\")\nplt.title(\"(a) Initial\")\nplt.ylabel(\"Count\")\nplt.hist(onp.array(starter_points.flatten()), bins=100)\n\nplt.sca(axes[1])\nplt.xlabel(\"Support\")\nplt.title(\"(b) Final\")\nplt.hist(onp.array(final.flatten()), bins=100)\n\nplt.sca(axes[2])\nplt.hist(onp.array(data), bins=100)\nplt.title(\"(c) Original Data\")\n\nsns.despine()\n\n\n\n\n\nFigure 3.4: (a) Initial and (b) final positions of points alongside (c) the original data.\n\n\n\n\nFigure 3.4 looks quite reasonable! Our original draws from a relatively wide Gaussian get split up into both component distribution which is encouraging here. This is encouraging!\nOne thing I hope is evident here is the vmap-ing of the the sampler over multiple starting points. For me, that is one of the elegant things about JAX. With vmap, lax.scan, and other primitives in place, as long as we can “stage out” the elementary units of computation by implementing them as callables (or functions), we have a very clear path to incorporating them in loopy constructs such as vmap and lax.scan, and JIT-compiling them using jit."
  },
  {
    "objectID": "notebooks/03-noise-scales.html#the-problem-with-naïve-score-approximation",
    "href": "notebooks/03-noise-scales.html#the-problem-with-naïve-score-approximation",
    "title": "4  Noise Scales",
    "section": "4.1 The problem with naïve score approximation",
    "text": "4.1 The problem with naïve score approximation\nWith a score function approximator, we have one small issue: in regions of low sample density, our estimate of the score function will be inaccurate, simply because we have few samples in those regimes. To get around this, we can:\n\nperturb data points with noise and train score-based models on the noisy data points instead. When the noise magnitude is sufficiently large, it can populate low data density regions to improve the accuracy of estimated scores.\n\nThere is a huge tradeoff here, though: the larger the amount of perturbation, the greater the corruption of the input data. Let’s see that in action.\nAs always, we’ll start by generating some mixture Gaussian data.\n\n\nCode\nfrom jax import random, vmap\nimport jax.numpy as np \nimport matplotlib.pyplot as plt\nimport numpy as onp \nimport seaborn as sns \n\ndef ecdf(data):\n    x, y = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n    return x, y\n\nkey = random.PRNGKey(45)\nk1, k2, k3 = random.split(key, 3)\n\nlocations = [-15, 0, 15]\n\nmix1 = random.normal(k1, shape=(1000,)) * 1 + locations[0]\nmix2 = random.normal(k2, shape=(500,)) * 1 + locations[1]\nmix3 = random.normal(k2, shape=(500,)) * 1 + locations[2]\n\ndata = np.concatenate([mix1, mix2, mix3]).reshape(-1, 1)\n\nfig, axes = plt.subplots(figsize=(8, 4), ncols=2, sharex=True)\n\nplt.sca(axes[0])\nplt.hist(onp.array(data), bins=100)\nplt.xlabel(\"Support\")\nplt.title(\"Histogram\")\n\nplt.sca(axes[1])\nplt.plot(*ecdf(data.flatten()))\nplt.xlabel(\"Support\")\nplt.ylabel(\"Cumulative Fraction\")\nplt.title(\"ECDF\")\n\nsns.despine()\n\n\n\n\n\nFigure 4.1: Histogram and ECDF of two-component mixture Gaussian data.\n\n\n\n\nIn this case, we have intentionally spaced out the Gaussians to create a region of extremely low density (in the approximate region \\((-5, 4)\\)). As we’ll see later, in this region, the gradients will be really hard to estimate, and the errors in this region may be pretty large.\nNext up, we’re going to perturb that data. What we do here is add standard Gaussian draws to each data point over 5 noising steps. Progressively, the draws should converge on a very smooth Gaussian.\n\nfrom functools import partial \nnoise_scale = np.linspace(1, 8, 9)\n\ndef noise(data, scale, key):\n    draw = random.normal(key, shape=data.shape) * scale \n    return data + draw \n\nkeys = random.split(k3, len(noise_scale))\n\ndata_perturbed = vmap(partial(noise, data))(noise_scale, keys)\ndata_perturbed.shape  # (num_noise_scales, num_samples, data_dim)\n\n(9, 2000, 1)\n\n\n\n\nCode\nfig, axes = plt.subplot_mosaic(\"\"\"\nABC\nDEF\nGHI\nJJJ\n\"\"\",\n    figsize=(8, 10.5), sharex=True)\n\nax_keys = \"ABCDEFGHI\"\nfor i, (row, scale, ax_key) in enumerate(zip(data_perturbed, noise_scale, ax_keys)):\n    plt.sca(axes[ax_key])\n    plt.hist(onp.array(row.flatten()), bins=100)\n    plt.title(f\"$\\sigma$={scale}\")\n    plt.xlabel(\"Support\")\n    for loc in locations: \n        plt.axvline(loc, color=\"black\", ls=\"--\")\n\nplt.sca(axes[\"J\"])\nfor row, scale in zip(data_perturbed, noise_scale):\n    plt.plot(*ecdf(row.flatten()), label=f\"$\\sigma$={scale}\")\n    for loc in locations: \n        plt.axvline(loc, color=\"black\", ls=\"--\")\n\nplt.legend()\nplt.xlabel(\"Support\")\nplt.ylabel(\"Cumulative Fraction\")\nplt.title(\"ECDF\")\n\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nFigure 4.2: Distribution of samples with and without perturbation. (Top three rows) Histograms of the data. (Bottom) Empirical cumulative distribution function of samples with differing levels of perturbation.\n\n\n\n\nShould be evident from the figure above that when we add more noise, the data look more and more like a single Gaussian and less like the original. Most crucially, in the regions of low density between the two mixture Gaussians (the flat regime in the blue line), we have a region of high density in the perturbed distributions (the red line in the same region). We should be able to obtain accurate score models for the perturbed data in the regime of low density support (on the blue curve). As we will see later, this will help us obtain slightly more accurate score models for the blue curve’s flat region. In theory, if we were to reverse the process, we should get back our original data distribution."
  },
  {
    "objectID": "notebooks/03-noise-scales.html#sampling-using-the-score-function",
    "href": "notebooks/03-noise-scales.html#sampling-using-the-score-function",
    "title": "4  Noise Scales",
    "section": "4.2 Sampling using the score function",
    "text": "4.2 Sampling using the score function\nOK! With this, we are now ready to start sampling! This is the logic that we’re going to follow. We know that the last perturbation’s score models are going to be more accurate on the perturbed distribution, but it’s also going to be less accurate about the original distribution. To recap, we’re going to need a score model that approximates the score function of our data and a Langevin dynamics sampler for generating new data.\n\n4.2.1 Train Score Model\nAs always, we train the score model.\n\nfrom score_models.training import fit\nfrom score_models.models import FeedForwardModel1D\nfrom score_models.losses import score_matching_loss\nfrom score_models.sampler import LangevinDynamicsChain\nfrom jax import nn\nimport optax\nfrom functools import partial\n\nffmodel = FeedForwardModel1D(depth=2, width_size=512, activation=nn.softplus)\noptimizer = optax.chain(\n    optax.adam(learning_rate=5e-3),\n)\n\nupdated_model, history = fit(\n    ffmodel, data, score_matching_loss, optimizer, 2_000, progress_bar=True\n)\n\n\n\n\n\n\n4.2.2 Sample\nThen, we sample new data using the score function coupled with a Langevin dynamics sampler.\n\nn_particles = 20_000\nstarter_points = random.normal(key, shape=(n_particles, 1, 1)) * 10\nstarter_keys = random.split(key, n_particles)\ntrained_model_sampler = LangevinDynamicsChain(\n    gradient_func=updated_model, n_samples=100, epsilon=5e-1\n)\nfinal_non_joint, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n\n\n\n4.2.3 Visualize the samples\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 3), ncols=3, nrows=1, sharex=True)\n\nplt.sca(axes[0])\nplt.plot(*ecdf(final_non_joint.flatten()), label=\"Sampled\")\nplt.plot(*ecdf(data.flatten()), label=\"Original\")\nfor loc in locations: \n    plt.axvline(loc, color=\"black\", ls=\"--\")\n\nplt.xlabel(\"Support\")\nplt.title(\"ECDF\")\nplt.legend()\n\nplt.sca(axes[1])\nplt.hist(onp.array(final_non_joint.flatten()), bins=100)\nfor loc in locations: \n    plt.axvline(loc, color=\"black\", ls=\"--\")\nplt.xlabel(\"Support\")\nplt.title(\"Samples\")\n\nplt.sca(axes[2])\nplt.hist(onp.array(data.flatten()), bins=100)\nfor loc in locations: \n    plt.axvline(loc, color=\"black\", ls=\"--\")\nplt.xlabel(\"Support\")\nplt.title(\"Data\")\n\nsns.despine()\n\n\n\n\n\nFigure 4.3: Final samples from 10,000 particles run for 100 sampling steps, shown as (a) a histogram, and (b) an ECDF alongside the original data.\n\n\n\n\nOK, yes, we’re able to! Looking at the distributions, notice how the mixture weights are a bit different between the samples and the data. However, we should notice that the middle Gaussian has a long tail of samples. This implies something wrong with gradient estimation. The problem here is either due to a lack of training budget (2,000 steps only) or because it is genuinely difficult to estimate gradients in that regime. The latter is what Yang Song states as the main issue.\nLet’s also do a side-by-side comparison of gradients for the original data v.s. the true score function of the mixture model.\n\n\nCode\nfrom score_models.models.gaussian import MixtureGaussian\n\nsupport = np.linspace(-20, 20, 1000).reshape(-1, 1)\ngradients = vmap(updated_model)(support)\nplt.plot(support, gradients, label=\"Estimated\")\n\ntrue_score_func = MixtureGaussian(\n    mus=np.array(locations),\n    log_sigmas=np.log(np.array([1.0, 1.0, 1.0])),\n    ws=np.array([0.5, 0.25, 0.25]),\n)\noriginal_gradients = vmap(true_score_func)(support).squeeze()\nplt.plot(support, original_gradients, label=\"True\")\nplt.legend()\n\nplt.title(\"Estimated Gradients\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\nplt.axhline(y=0, color=\"black\", ls=\"--\")\nplt.legend()\nsns.despine()\n\n\n\n\n\nFigure 4.4: Estimated gradients for the original data.\n\n\n\n\nThe gradients diagnostic here should also be quite illuminating. In particular, there are regions where the gradient estimation is way off. That is where the next point might come in handy."
  },
  {
    "objectID": "notebooks/03-noise-scales.html#one-score-model-per-perturbation",
    "href": "notebooks/03-noise-scales.html#one-score-model-per-perturbation",
    "title": "4  Noise Scales",
    "section": "4.3 One score model per perturbation",
    "text": "4.3 One score model per perturbation\nOne key idea in Yang Song’s blog post is that we can jointly train score models for each of the noise levels and then use Langevin dynamics in an annealed fashion to progressively obtain better and better samples from the original data distribution. The loss function here is a weighted sum of Fisher divergences, or, more simply, a sum of score model losses weighted by the noise scale applied to the data. The intuition here is that we weigh more heavily the strongly perturbed data and weigh less heavily the weakly perturbed data, because the score model will be more accurate for the strongly perturbed data. Thinking downstream two steps, we will be using a procedure called annealed Langevin dynamics to sample from this mixture Gaussian, such that In our example, we will have a batch of models trained with a single loss function, one for each scale value, which is the weighted sum of Fisher divergences,\n\nfrom functools import partial \n\nFirstly, we start with a vmap-ed version of our model. This will make it easy for us to train a batch of models together.\n\nfrom pyprojroot import here \nimport cloudpickle as pkl\n\ndef make_model(key):\n    ffmodel = FeedForwardModel1D(\n        depth=2, width_size=512, activation=nn.softplus, key=key\n    )\n    return ffmodel\n\nkey = random.PRNGKey(49)\nkeys = random.split(key, len(noise_scale))\nmodels = []\nfor key in keys:\n    models.append(make_model(key))\n\nNext, we define our joint loss function. Here, the loss function is a weighted sum of score matching losses. In related body of work, the greater the noise scale, the higher the weight. The intuition here is that gradients are more accurately estimated at higher noise scales, while gradients are less accurately estimated at lower noise scales. For fairness in comparison, we will use the same number of training steps are before for independently-trained models.\n\nfrom score_models.losses import joint_score_matching_loss\n\noptimizer = optax.chain(\n    optax.clip(0.1),\n    optax.adam(5e-3),\n)\n\n(here() / \"artifacts\").mkdir(exist_ok=True)\n\nn_steps = 2_000\n\nartifact_path = here() / f\"artifacts/noise_scale_model_{n_steps}.pkl\"\nupdated_models, training_history = fit(\n    models,\n    data_perturbed,\n    partial(joint_score_matching_loss, scales=noise_scale),\n    optimizer=optimizer,\n    steps=n_steps,\n)\nwith open(artifact_path, \"wb\") as f:\n    pkl.dump((updated_models, training_history), f)"
  },
  {
    "objectID": "notebooks/03-noise-scales.html#confirm-sampling-works-after-joint-training",
    "href": "notebooks/03-noise-scales.html#confirm-sampling-works-after-joint-training",
    "title": "4  Noise Scales",
    "section": "4.4 Confirm sampling works after joint training",
    "text": "4.4 Confirm sampling works after joint training\nWe’re now going to do a quick sanity-check: our trained score models should be usable to sample from the mixture distribution. Let’s confirm that before proceeding.\n\nn_particles = 1_000\nstarter_points = random.normal(key, shape=(n_particles, 1, 1)) * 10\nstarter_keys = random.split(key, n_particles)\ntrained_model_sampler = LangevinDynamicsChain(\n    gradient_func=updated_models[0], n_samples=100, epsilon=5e-1\n)\nfinal, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 3), ncols=3, nrows=1, sharex=True)\n\nplt.sca(axes[0])\nplt.plot(*ecdf(final.flatten()), label=\"Sampled\")\nplt.plot(*ecdf(data.flatten()), label=\"Original\")\nplt.xlabel(\"Support\")\nplt.title(\"ECDF\")\nplt.legend()\n\nplt.sca(axes[1])\nplt.hist(onp.array(final.flatten()), bins=100)\nplt.xlabel(\"Support\")\nplt.title(\"Samples\")\n\nplt.sca(axes[2])\nplt.hist(onp.array(data.flatten()), bins=100)\nplt.xlabel(\"Support\")\nplt.title(\"Data\")\n\nsns.despine()\n\n\n\n\n\nFigure 4.5: Sanity check of sampling. (left) ECDF of samples vs. original data. Histogram of (middle) samples and (right) original data are also shown."
  },
  {
    "objectID": "notebooks/03-noise-scales.html#annelaed-langevin-dynamics-sampling",
    "href": "notebooks/03-noise-scales.html#annelaed-langevin-dynamics-sampling",
    "title": "4  Noise Scales",
    "section": "4.5 Annelaed Langevin Dynamics Sampling",
    "text": "4.5 Annelaed Langevin Dynamics Sampling\nAnnealed Langevin dynamics sampling is a way to get around the problem of poorly estimated gradients in low density regions. The procedure is rather simple and elegant. We start by performing Langevin dynamics sampling at the highest noise value. After a fixed number of steps, we freeze the samples and use them as the starting point for sampling at the next highest noise value, progressively stepping down the noise until we hit the unperturbed data. In doing so, we ensure that the score function can be progressively estimated around regions of high density while progressively worrying less and less about the low density gradients.\n\nn_particles = 10_000\nstarter_points = random.normal(key, shape=(n_particles, 1, 1)) * 10\nstarter_keys = random.split(key, n_particles)\n\nfinal_points_history = []\nfor model in updated_models[::-1]:\n    trained_model_sampler = LangevinDynamicsChain(\n        gradient_func=model, n_samples=100, epsilon=5e-1\n    )\n    final_points, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n    final_points_history.append(final_points)\n    starter_points = final_points\n\n\n\nCode\nfig, axes = plt.subplots(figsize=(8, 4), ncols=5, nrows=2, sharey=False)\naxes = axes.flatten()\n\nfor ax, history, scale in zip(axes, final_points_history, noise_scale[::-1]):\n    plt.sca(ax)\n    plt.hist(onp.array(history.flatten()), bins=100)\n    plt.title(f\"Scale: {scale:.1f}\")\n    for loc in locations:\n        plt.axvline(x=loc, ls=\"--\", color=\"black\")\n\nplt.sca(axes[-1])\nplt.hist(onp.array(data.flatten()), bins=100)\nplt.title(\"Data\")\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nFigure 4.6: Annealed Langevin dynamics samples at varying noise scales. Data (bottom-right) is shown for comparison.\n\n\n\n\nLet’s also compare samples taken from jointly trained vs. independently trained models alongside their estimated score functions.\n\n\nCode\nsupport = np.linspace(-20, 20, 1000).reshape(-1, 1)\n\nfig, axes = plt.subplot_mosaic(\"\"\"\nAD\nAD\nBE\nBE\nCF\nCF\n\"\"\", figsize=(8, 8))\n# First subplot: show gradients for noise scale 1.0 and for ground truth.\nplt.sca(axes[\"A\"])\ntrue_score = vmap(true_score_func)(support.squeeze())\nplt.plot(support, true_score, label=\"True\")\nplt.title(\"True Score\")\nplt.ylabel(\"Score\")\n\nplt.sca(axes[\"B\"])\nindependent_score = vmap(updated_model)(support)\nplt.plot(support, independent_score, label=\"Independent\")\nplt.title(\"Independently-Trained\\nEstimated Score\")\nplt.ylabel(\"Score\")\n\nplt.sca(axes[\"C\"])\njoint_score = vmap(updated_models[0])(support)\nplt.plot(support, joint_score, label=\"Joint\")\nplt.title(\"Jointly-Trained\\nEstimated Score\")\nplt.xlabel(\"Support\")\nplt.ylabel(\"Score\")\n\nplt.sca(axes[\"D\"])\nplt.hist(onp.array(data.flatten()), bins=100)\nplt.title(\"True Distribution Samples\")\n\nplt.sca(axes[\"E\"])\nplt.hist(onp.array(final_non_joint.flatten()), bins=100)\nplt.title(\"Independently-Trained Samples\")\n\nplt.sca(axes[\"F\"])\nplt.hist(onp.array(final_points_history[-1].flatten()), bins=100)\nplt.title(\"Annealed Langevin\\nDynamics Samples\")\nplt.xlabel(\"Support\")\n\nfor axname, ax in axes.items():\n    plt.sca(ax)\n    for loc in locations:\n        plt.axvline(x=loc, ls=\"--\", color=\"black\")\n\nplt.tight_layout()\nsns.despine()\n\n\n\n\n\nFigure 4.7: Comparison between true (top), independently trained (middle) and jointly trained (bottom) score models. (left) Score models evaluated along the support. (right) Samples drawn from true distribution (right-top) and by Langevin dynamics sampling (right middle and right bottom).\n\n\n\n\nWhile it’s tempting to look at the peaks, the region of low density is where we should focus our attention. In the middle row’s samples, we see that the independently-trained model has a long tail of density between the middle component and the right component, something that is missing in the true distribution’s draws and in the annealed samples. The same can be seen on the left side, where we see the estimated score function taking on a rather inaccurate shape in that low density regime."
  },
  {
    "objectID": "notebooks/04-diffeq.html#ordinary-differential-equations",
    "href": "notebooks/04-diffeq.html#ordinary-differential-equations",
    "title": "5  Differential Equations",
    "section": "5.1 Ordinary differential equations",
    "text": "5.1 Ordinary differential equations\nLet’s start with ODEs. ODEs are usually taught in undergraduate calculus classes, since they involve differentiation and integration. I do remember encountering them while studying in secondary school and junior college in Singapore, which is a testament to how advanced the mathematics curriculum in Singapore is.\nODEs are useful models of systems where we believe that the rate of change of an output variable is a math function of some input variable. In abstract mathematical symbols:\n\\[\\frac{dy}{dx} = f(x, \\theta)\\]\nHere, \\(f\\) simply refers to some mathematical function of \\(x\\) and the function’s parameters \\(\\theta\\).\n\n5.1.1 A classic ODE example\nA classic ODE example that we might think of is that of a decay curve:\n\\[\\frac{dy}{dt} = -y\\]\nImplemented in diffrax, which is a JAX package for differential equations, and wrapped in Equinox as a parameterized function, we have the following code:\n\nfrom diffrax import diffeqsolve, Tsit5, ODETerm, SaveAt\nimport jax.numpy as np\nfrom jax import vmap\nimport equinox as eqx\n\ndef exponential_decay_drift(t: float, y: float, args: tuple):\n    \"\"\"Exponential decay drift term.\"\"\"\n    return -y\n\n\nfrom score_models.models.ode import ODE\nfrom inspect import getsource \n\nprint(getsource(ODE))\n\nclass ODE(eqx.Module):\n    \"\"\"Equinox ODE module.\n\n    Wraps a very common ODE code pattern into a single object.\n    \"\"\"\n\n    drift: callable\n\n    def __call__(self, ts: np.ndarray, y0: float) -&gt; np.ndarray:\n        \"\"\"Solve an ODE model.\n\n        :param ts: Time steps to follow through.\n        :param y0: Initial value.\n        :returns: The trajectory starting from y0.\n        \"\"\"\n        term = ODETerm(self.drift)\n        solver = Tsit5()\n        saveat = SaveAt(ts=ts, dense=True)\n        sol = diffeqsolve(\n            term, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat\n        )\n        return vmap(sol.evaluate)(ts)\n\n\n\nFor those of us who have learned about ODEs, the structure of the code above should look pretty familiar. The diffrax API neatly organizes what we need to solve ODEs:\n\nthe ODETerm, which is the \\(\\frac{dy}{dt}\\) equation,\na solver, for which diffrax provides a library of them,\nthe initial and end points \\(t_0\\) and \\(t_1\\) along the \\(t\\) axis along with step size \\(dt\\),\nthe initial value of \\(y\\), i.e. \\(y_0\\).\n\nFinally, when calling on the ODE, we evaluate the solution path from the starting time to the ending time, given an initial starting value.\n\node = ODE(exponential_decay_drift)\nts = np.linspace(0, 10, 1000)\nys = ode(ts=ts, y0=3)\n\n\n\nCode\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nplt.plot(ts, ys)\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nsns.despine()\n\n\n\n\n\nFigure 5.1: Solution to the ODE \\(f'(y) = -y\\).\n\n\n\n\nThe solution of the ODE that we had above is an exponential decay, and that is exactly what we see in the curve above.\nAnd if we wanted to run the ODE from multiple starting points:\n\n\nCode\nys = ode(ts=ts, y0=np.arange(-10, 10))\n\nfor curve in ys.T:\n    plt.plot(ts, curve, color=\"blue\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nsns.despine()\n\n\n\n\n\nFigure 5.2: Multiple solutions to the ODE \\(f'(y) = -y\\)."
  },
  {
    "objectID": "notebooks/04-diffeq.html#stochastic-differential-equations",
    "href": "notebooks/04-diffeq.html#stochastic-differential-equations",
    "title": "5  Differential Equations",
    "section": "5.2 Stochastic Differential Equations",
    "text": "5.2 Stochastic Differential Equations\nStochastic differential equations (SDEs) extend ODEs by adding in noise into each step. SDEs can thus be thought of as having a “drift” component, in which the system being modeled by the SDE “drifts” through the vector field, and a “diffusion” component, in which the system’s state is perturbed with additional noise. SDEs have the general form:\n\\[dx = f(x, t)dt + g(t)dw\\]\nTo paraphrase Yang’s blog post, here are the definitions of each of the terms.\n\n\\(f(x, t)\\) is a drift function that produces a vector output, i.e. what would have been the ODE term. This term controls the “drift”-ing of the system in observed data space.\n\\(g(t)\\) is a diffusion function that produces a scalar output, i.e. the scalar multiplier of \\(dw\\). This term adds “diffusive” noise to the output.\n\\(dw\\) is the infinitesimal white noise term.\n\n\\(f(x, t)dt\\) is usually referred to as the “ODE Term”, while \\(g(t)dw\\) is usually referred to as the “Control Term”. We can see that in the implementation of the SDE module below.\n\nfrom score_models.models.sde import SDE\n\nprint(getsource(SDE))\n\nclass SDE(eqx.Module):\n    \"\"\"Equinox SDE module.\n\n    Wraps a very common SDE code pattern into a single object.\n    \"\"\"\n\n    drift: callable\n    diffusion: callable\n\n    def __call__(self, ts: np.ndarray, y0: float, key: random.PRNGKey) -&gt; np.ndarray:\n        \"\"\"Solve an SDE model.\n\n        :param ts: Time steps to follow through.\n        :param y0: Initial value.\n        :param key: PRNG key for reproducibility purposes.\n        :returns: The trajectory starting from y0.\n        \"\"\"\n        brownian_motion = VirtualBrownianTree(\n            ts[0], ts[-1], tol=1e-3, shape=(), key=key\n        )\n        terms = MultiTerm(\n            ODETerm(self.drift), ControlTerm(self.diffusion, brownian_motion)\n        )\n        solver = Heun()\n        saveat = SaveAt(t0=True, ts=ts, dense=True)\n        sol = diffeqsolve(\n            terms, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat\n        )\n        return vmap(sol.evaluate)(ts)\n\n\n\n\n5.2.1 Noisy Decay\nFor illustration, let’s see what happens we we apply homoskedastic noise to the decay process. Here, homoskedastic noise refers to a noise term that is independent of time. Firstly, we have it defined in code.\n\nfrom functools import partial\n\ndef homoskedastic_diffusion(t, y, args):\n    \"\"\"Time-independent noise.\"\"\"\n    return 0.3\n\nNext, we set up the SDE and solve it going forward in time.\n\nfrom jax import random \n\nn_timesteps = 17\nn_starting = 1001\n\ndemo_key = random.PRNGKey(55)\n\ny0_key, key = random.split(demo_key)\ny0s = random.normal(y0_key, shape=(n_starting,))  # We solve the SDE for each draw from a Guassian.\n\nsde_keys = random.split(key, len(y0s))\nts = np.linspace(0, 4, n_timesteps)\nsde = SDE(drift=exponential_decay_drift, diffusion=homoskedastic_diffusion)\nsde = partial(sde, ts)\nys = vmap(sde)(y0s, sde_keys)\n\nNow, let’s plot the trajectories:\n\n\nCode\nfor y in ys:\n    plt.plot(ts, y, alpha=0.01, color=\"blue\")\n\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nsns.despine()\n\n\n\n\n\nFigure 5.3: SDE with exponential decay drift and homoskedastic disffusion.\n\n\n\n\n\n\n5.2.2 Oscillating SDE\nLet’s do another example: oscillating SDEs! Here, we have an oscillating system (cosine drift) in which we add a homoskedastic diffusion term.\n\ndef cosine_drift(t, y, args):\n    return np.cos(t)\n\n\nsde = SDE(drift=cosine_drift, diffusion=homoskedastic_diffusion)\nts_oscillating = np.linspace(1, 10, n_timesteps)\nsde = partial(sde, ts_oscillating)\nkeys = random.split(key, 1001)\noscillating_y0s = random.normal(key, shape=(1001,)) * 0.1\noscillating_ys = vmap(sde)(oscillating_y0s, keys)\n\nLikewise, let’s plot this one too:\n\n\nCode\nfor y in oscillating_ys:\n    plt.plot(ts_oscillating, y, color=\"blue\", alpha=0.01)\n\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nsns.despine()\n\n\n\n\n\nFigure 5.4: SDE with exponential decay drift and homoskedastic disffusion.\n\n\n\n\nAt each timepoint, there is also a marginal distribution. Let’s inspect that below.\n\n\nCode\nimport numpy as onp\nfig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\naxes = axes.flatten()\n\nfor ax, t, y in zip(axes, ts_oscillating, oscillating_ys.T):\n    plt.sca(ax)\n    plt.hist(onp.array(y), bins=30)\n    plt.title(f\"time={t:.1f}\")\n\nsns.despine()\nplt.delaxes(axes[-1])\nplt.tight_layout()\n\n\n\n\n\nFigure 5.5: Marginal distribution at each time point of the oscillating SDE.\n\n\n\n\n\n\n5.2.3 Noising SDE\nFor the purposes of noising up date, we would want an SDE that noises up data with increasing amounts of noise with time. Here, we can design the SDE such that the drift would be 0 at all time points, while the diffusion term would be some multiplier on time.\n\ndef constant_drift(t, y, args):\n    \"\"\"Constant drift term.\"\"\"\n    return 0\n\ndef time_dependent_diffusion(t, y, args):\n    \"\"\"Diffusion term that increases with time.\"\"\"\n    return 0.3 * t\n\nAs always, we set up the SDE:\n\nsde = SDE(drift=constant_drift, diffusion=time_dependent_diffusion)\nts_noising = np.linspace(0, 4, n_timesteps)\nsde = partial(sde, ts_noising)\ny0s = random.normal(key, shape=(n_starting,)) * 0.1  # we start with N(0, 0.1) draws.\nkeys = random.split(key, n_starting)\nnoising_ys = vmap(sde)(y0s, keys)\n\nThen let’s plot the solved SDE trajectories:\n\n\nCode\nfor y in noising_ys:\n    plt.plot(ts_noising, y, color=\"blue\", alpha=0.01)\n    plt.xlabel(\"t\")\n    plt.ylabel(\"y\")\n    plt.title(f\"{n_starting} sample trajectories\")\n    sns.despine()\n\n\n\n\n\nFigure 5.6: A “noising” SDE that progressively adds more noise over time.\n\n\n\n\nAs we can see above, we are able to obtain greater amounts of noise from a tight starting point. We can verify that by looking at the marginal distributions.\n\n\nCode\nimport numpy as onp\nfig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\naxes = axes.flatten()\n\nfor ax, t, y in zip(axes, ts_noising, noising_ys.T):\n    plt.sca(ax)\n    plt.hist(onp.array(y), bins=30)\n    plt.title(f\"time={t:.1f}, σ={onp.std(y):.2f}\")\n\nsns.despine()\nplt.delaxes(axes[-1])\nplt.tight_layout()\n\n\n\n\n\nFigure 5.7: Marginal distribution at each time point of a noising SDE.\n\n\n\n\nFrom the marginal distributions at each noise timestep, we see that we indeed have ever increasing amounts of noise. (Note how the x-axis scale is the same on all of the plots.) The empirical standard deviation from the mean is also shown on the plots above."
  },
  {
    "objectID": "notebooks/04-diffeq.html#reverse-time-sdes",
    "href": "notebooks/04-diffeq.html#reverse-time-sdes",
    "title": "5  Differential Equations",
    "section": "5.3 Reverse Time SDEs",
    "text": "5.3 Reverse Time SDEs\nWith constant drift and time-dependent diffusion, we can noise up data in a continuous fashion. How do we go backwards? Here is where solving the reverse time SDE will come in. Again, we need to set up the drift and diffusion terms. Here, the drift term is:\n\\[f(x, t) - g^2(t) \\nabla_x \\log p_t (x) \\]\nwhere:\n\n\\(f(x, t)\\) is the drift term of the forward SDE,\n\\(g(t)\\) is the diffusion term of the forward SDE, and\n\\(\\nabla_x \\log p_t (x)\\) is the score function of the data.\n\nAnd the diffusion term is:\n\\[g(t) dw\\]\nwhich is basically the diffusion term of the forward SDE.\nHowever, the tricky part here is that we don’t have access to \\(\\nabla_x \\log p_t (x)\\) (the true score function). As such, we need to bring out our score model approximator! To train the score model approximator, we need the analogous score matching objective for continuous time problems."
  },
  {
    "objectID": "notebooks/04-diffeq.html#continuous-time-score-models",
    "href": "notebooks/04-diffeq.html#continuous-time-score-models",
    "title": "5  Differential Equations",
    "section": "5.4 Continuous Time Score Models",
    "text": "5.4 Continuous Time Score Models\nIn an ideal situation, we would train the score matching model using a weighted combination of Fisher divergences:\n\\[\\mathbb{E}_{t \\in U(0, T)} \\mathbb{E}_{p_t(x)} [ \\lambda(t) || \\nabla_x \\log p_t(x) - s_{\\theta}(x, t) ||^2_2]\\]\nNow, just like before, we don’t have access to \\(\\nabla_x \\log p_t (x)\\), so we instead use the score matching objective by Hyvärinen (Hyvärinen 2005). What’s really cool here is that we can train the models using the noised up data. The protocol is basically as follows:\n\nNoise up our original data using an SDE.\nTrain score models to estimate the score function of the noised up data.\nUse the approximate score function to calculate the reverse-time SDE.\n\n\n5.4.1 Model implementation\nTo get this right, we need a score function approximator that is compatible with SDEs, i.e. they accept both x and t as part of the function signature and return the gradient value.\nscore: float = score_model(x, t) \nLet’s implement it below:\n\nfrom jax import jacfwd\nfrom jax import nn\nimport equinox as eqx\n\nclass SDEFeedForwardModel1D(eqx.Module):\n    \"\"\"Time-dependent score model.\n    \n    We choose an MLP here with 2 inputs (`x` and `t` concatenated),\n    and output a scalar which is the estimated score.\n    \"\"\"\n\n    mlp: eqx.Module\n\n    def __init__(\n        self,\n        in_size=2,\n        out_size=1,\n        width_size=256,\n        depth=1,\n        activation=nn.softplus,\n        key=random.PRNGKey(45),\n    ):\n        self.mlp = eqx.nn.MLP(\n            in_size=in_size,\n            out_size=out_size,\n            width_size=width_size,\n            depth=depth,\n            activation=activation,\n            key=key,\n        )\n\n    @eqx.filter_jit\n    def __call__(self, x: float, t: float):\n        \"\"\"Forward pass.\n\n        :param x: Data. Should be of shape (1, :),\n            as the model is intended to be vmapped over batches of data.\n        :returns: Estimated score of a Gaussian.\n        \"\"\"\n        if isinstance(x, float) or x.ndim == 0:\n            x = np.array([x])\n        if isinstance(t, float) or x.ndim == 0:\n            t = np.array([t])\n        x = np.array([x.squeeze(), t.squeeze()])\n        return self.mlp(x).squeeze()\n\nThere are a few design notes for the model above that we’d like to note.\nFirstly, note how its structure is essentially identical to the neural net score model from before, i.e. a multi-layer perceptron, except that now it takes in both x and t as its inputs. This is important because we are no longer interested in a discrete score model, with one per time point. Instead, we are interested in a score model that can estimate the score function of our noised up data at any time point along the SDE-based continuous-time noising function.\nLet’s now instantiate the model:\n\nmodel = SDEFeedForwardModel1D(\n    width_size=256, depth=2, activation=nn.softplus, key=random.PRNGKey(55)\n)\n\n\n\n5.4.2 Loss function\nNow, we need to calculate the loss for a batch of data. As with before, we need a score-matching loss for each noise level (i.e. for each t). In here, we have the model’s dependence on time (i.e. noise level) encoded as part of the model structure.\n\nfrom score_models.losses.sde import score_matching_loss as sde_score_matching_loss, joint_score_matching_loss as sde_joint_score_matching_loss\n\nprint(getsource(sde_score_matching_loss))\n\ndef score_matching_loss(\n    model: Union[eqx.Module, Callable], noised_data: np.ndarray, t: float\n) -&gt; float:\n    \"\"\"Score matching loss for SDE-based score models.\n\n    :param model: Equinox model.\n    :param noised_data: Batch of data from 1 noise scale of shape (batch, n_data_dims).\n    :param t: Time in SDE at which the noise scale was evaluated.\n    :returns: Score matching loss for one batch of data.\n    \"\"\"\n    model = partial(model, t=t)\n    dmodel = jacfwd(model, argnums=0)\n    term1 = vmap(dmodel)(noised_data)\n    if term1.ndim &gt; 1:\n        term1 = vmap(np.diagonal)(term1)\n    term2 = 0.5 * vmap(model)(noised_data) ** 2\n    inner_term = term1 + term2\n    summed_by_dims = vmap(np.sum)(inner_term)\n    return np.mean(summed_by_dims)\n\n\n\n\nprint(getsource(sde_joint_score_matching_loss))\n\n@eqx.filter_jit\ndef joint_score_matching_loss(\n    model: Union[eqx.Module, Callable], noised_data_all: np.ndarray, ts: np.ndarray\n):\n    \"\"\"Joint score matching loss.\n\n    :param model: An equinox model.\n    :param noised_data_all: An array of shape (time, batch, n_data_dims).\n    :param ts: An array of shape (time,).\n    :returns: Score matching loss, summed across all noise scales.\n    \"\"\"\n    loss_score = 0\n    for noise_batch, t in zip(noised_data_all, ts):\n        scale = t\n        loss_score += score_matching_loss(model, noise_batch, t) * scale\n    return loss_score\n\n\n\n\n\n5.4.3 Train score model\nNow, let’s train the score model on our noised up data. Below, we have our training loop.\n\nimport optax\nfrom tqdm.auto import tqdm\n\nfrom score_models.losses import joint_sde_score_matching_loss, sde_score_matching_loss\n\nmodel = SDEFeedForwardModel1D(key=random.PRNGKey(55))\n\noptimizer = optax.chain(\n    optax.adam(5e-4),\n    # optax.clip(1e-6)\n)\n\nopt_state = optimizer.init(eqx.filter(model, eqx.is_array))\ndloss = eqx.filter_jit(eqx.filter_value_and_grad(joint_sde_score_matching_loss))\n\nn_steps = 13_000\niterator = tqdm(range(n_steps))\nloss_history = []\nupdated_score_model = model\nfor step in iterator:\n    loss_score, grads = dloss(updated_score_model, noising_ys.T, ts_noising)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    updated_score_model = eqx.apply_updates(updated_score_model, updates)\n    iterator.set_description(f\"Score: {loss_score:.2f}\")\n    loss_history.append(float(loss_score))\n\n\n\n\nLet’s also make sure that the model training has converged.\n\n\nCode\nplt.plot(loss_history)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss Score\")\nsns.despine()\n\n\n\n\n\nFigure 5.8: Training loss curve for our continuous-time score model.\n\n\n\n\n\n\n5.4.4 Sanity-check: score estimators match up with alternate calculation\nBecause we started with Gaussian noise and expanded the noise outwards, we still have Gaussians. Let’s check that the scores match a Gaussian’s score fitted onto the marginal distributions at each timepoint.\n\n\nCode\nfrom jax.scipy.stats import norm \nfrom jax import grad \n\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(8.5, 11))\n\nfor t, noised_ys, ax in zip(ts_noising, noising_ys.T, axes.flatten()):\n    plt.sca(ax)\n    noised_ys_mu = np.mean(noised_ys)\n    noised_ys_sigma = np.std(noised_ys)\n\n    logp_func = partial(norm.logpdf, loc=noised_ys_mu, scale=noised_ys_sigma)\n    np.sum(logp_func(noised_ys))\n    dlogp_func = grad(logp_func)\n\n    support = np.linspace(noised_ys_mu - noised_ys_sigma * 3, noised_ys_mu + noised_ys_sigma * 3, 1000)\n    estimated_score = vmap(dlogp_func)(support)\n    approximated_score = vmap(partial(updated_score_model, t=t))(support)\n    plt.plot(support, estimated_score, color=\"blue\", label=\"estimated\")\n    plt.plot(support, approximated_score, color=\"red\", label=\"approximated\")\n    plt.xlabel(\"Support\")\n    plt.ylabel(\"Score\")\n    plt.title(f\"t={t:.2f}\")\n    plt.tight_layout()\n\nplt.sca(axes.flatten()[0])\nplt.legend()\nsns.despine()\n\n\n\n\n\nFigure 5.9: Estimated (blue) vs. approximated (red) score functions at each time evaluation. Estimated score comes from taking the location (mean) and scale (stdev) of the observed data, while approximated score comes from the time-based score model.\n\n\n\n\nAs seen in Figure 5.9, it looks like our score model is able to approximate a time-dependent score function! The score function is least well-approximated within the region of 2 sigmas of support, even if not across the full 3 sigmas. This is encouraging. We should also note that the t=4.00 timepoint is the least well-approximated compared to the t=1.00 timepoint.\nNow, we’re going to look at the reverse drift. In an SDE, the drift term dictates where the system is going to move towards in the next time step. Let’s plot the vector field evaluated at each time step t.\n\ndef reverse_drift(t: float, y: float, args: tuple):\n    f = constant_drift(t, y, args)\n    g = time_dependent_diffusion(t, y, args)\n    s = updated_score_model(y, t)\n    return f - 0.5 * g**2 * s\n\nLet’s plot each of these four terms to make sure we get a good feel for what’s going on.\n\n\nCode\nimport pandas as pd \nfrom tqdm.auto import tqdm \n\n# Plot constant drift as a function of y and t.\nys = np.linspace(-3, 3, 50)\nts = np.linspace(0, 5, 50)\n\nfunction_evals = []\nfor yval in ys:\n    for t in ts:\n        dd = dict()\n        dd[\"constant_drift\"] = constant_drift(y=yval, t=t, args=())\n        dd[\"time_dependent_diffusion\"] = time_dependent_diffusion(y=yval, t=t, args=())\n        dd[\"score_approximation\"] = updated_score_model(x=yval, t=t)\n        dd[\"reverse_drift\"] = reverse_drift(y=yval, t=t, args=())\n        dd[\"y\"] = (yval)\n        dd[\"t\"] = t \n        function_evals.append(dd)\n\ncolumns = [\"constant_drift\", \"time_dependent_diffusion\", \"score_approximation\", \"reverse_drift\"]\n\nfunction_df = pd.DataFrame(function_evals)\nfor column in function_df.columns:\n    function_df[column] = function_df[column].astype(float)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\nfor ax, col in zip(axes.flatten(), columns):\n    function_eval = function_df[[\"y\", \"t\", col]].pivot_table(index=\"y\", columns=\"t\", values=col)\n    sns.heatmap(function_eval, ax=ax, cmap=\"viridis\")\n    ax.set_title(col)\n    ax.set_xticklabels([f\"{float(i._text):.2f}\" for i in ax.get_xticklabels()])\n    ax.set_yticklabels([f\"{float(i._text):.2f}\" for i in ax.get_yticklabels()])\n\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nFigure 5.10: Heatmap of each of the component functions in reverse_drift.\n\n\n\n\n\nconstant_drift is always 0, so no problem.\ntime_dependent_diffusion shows how diffusion increases over time, independent of the value of y, which is also correct, so no problem.\nscore_approximation shows how the Gaussian score approximator gives a gradient that is positive-valued when y is negative and vice versa, which pushes us towards region of high density. Also correct.\nreverse_drift shows us something interesting. We will end up with exploding values because +ve values drift more positive, while -ve values drift more -ve, until we hit very small time steps, and then we have no directional drift.\n\nNow, we could, in theory, run the SDE in reverse, but in my own testing of the equations, I found that I would encounter numerical stability issues. Because of the positive and negative drift zones in the reverse drift, we would end up getting extremely large negative or positive numbers. Hence, I skipped over solving the reverse SDE and instead went straight to probability flow ODEs, which are the very, very exciting piece of this entire body of work."
  },
  {
    "objectID": "notebooks/04-diffeq.html#probability-flow-odes",
    "href": "notebooks/04-diffeq.html#probability-flow-odes",
    "title": "5  Differential Equations",
    "section": "5.5 Probability Flow ODEs",
    "text": "5.5 Probability Flow ODEs\nNow that we’ve recapped what an ODE is, and have seen what SDEs can do for noising up our data, we can move on to probability flow ODEs. Why are these important? It’s because probability flow ODEs provide a deterministic mapping from our noise distribution to our data distribution and vice versa. In doing so, we can basically get rid of Langevin dynamics sampling and replace it entirely with a probability flow ODE instead. Before we go on, though, let’s take a quick look the key ODE that we need to solve:\n\\[dx = [f(x,t) - \\frac{1}{2} g^2(t) \\nabla_x \\log p_t (x)] dt\\]\nJust like the SDE above, the terms carry the same meaning:\n\n\n\\(f(x, t)\\) is a drift function that produces a vector output,\n\\(g(t)\\) is a diffusion function that produces a scalar output,\n\\(\\nabla_x \\log p_t (x)\\) is the score function, also replaceable by our neural net approximator\nand \\(dw\\) is infinitesimal white noise.\n\n(paraphrased from Yang’s blog)\n\nNow, if you study this carefully… the drift term is exactly the drift term we defined above! That means if we solve for the reverse drift ODE, we will get a path traced from the noise distribution back to the orginal data distribution! Let’s see that in action.\n\n\nCode\node_combined = ODE(reverse_drift)\n\node = ode_combined\nts = np.linspace(5, 0, 1000)\nkey = random.PRNGKey(55)\ny0s = np.linspace(-5 ,5, 10)\n\nfor y0 in y0s:\n    ys = ode(ts, y0)\n    plt.plot(ts, ys)\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nplt.gca().invert_xaxis()\nsns.despine()\n\n\n\n\n\nFigure 5.11: Probability flow ODE from noise distribution (t=5) back to original data (t=0)."
  },
  {
    "objectID": "notebooks/04-diffeq.html#generalizing-to-a-mixture-distribution",
    "href": "notebooks/04-diffeq.html#generalizing-to-a-mixture-distribution",
    "title": "5  Differential Equations",
    "section": "5.6 Generalizing to a Mixture Distribution",
    "text": "5.6 Generalizing to a Mixture Distribution\nThus far, we saw a probability flow ODE in action for a univariate, unimodal distribution. Let’s see if we can make the thing work for a mixture distribution.\n\n5.6.1 Generate Mixture Data\nWe will start by generating a mixture Gaussian dataset.\n\nmu1, sigma1 = -2.0, 1.0\nmu2, sigma2 = 2.0, 0.5\n\nk1, k2 = random.split(key)\n\ncomp1 = random.normal(k1, shape=(1000,)) * sigma1 + mu1 \ncomp2 = random.normal(k2, shape=(1000,)) * sigma2 + mu2\n\ndata = np.concatenate([comp1, comp2])\n\nplt.hist(data, bins=100);\n\n\n\n\nAnd just like that, we have:\n\nAn SDE that noises up data (forward-time),\nA score model that estimates the score of the data, and\nA probability flow ODE that maps from noise back to data (reverse-time).\n\nThat last point is the coolest of them all, in my opinion. Previously, we used Langevin dynamics sampling to sample out new sequences. While random sampling is simple, it also aesthetically felt less elegant than what we have with an ODE. The key ingredients here are:\n\nA time-dependent score model that can calculate (or estimate) the score of our noised distribution,\nThe known noise generator “diffusion” function, and\nThe known drift function,\n\nand all we need to do is solve a neural ODE while reversing time. Then, by drawing new coordinates from the noise distribution, we can deterministically map them back to the original data space!"
  },
  {
    "objectID": "notebooks/04-diffeq.html#references",
    "href": "notebooks/04-diffeq.html#references",
    "title": "5  Differential Equations",
    "section": "5.7 References",
    "text": "5.7 References\n\n\n\n\nHyvärinen, Aapo. 2005. “Estimation of Non-Normalized Statistical Models by Score Matching.” Journal of Machine Learning Research 6 (24): 695–709. http://jmlr.org/papers/v6/hyvarinen05a.html."
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#data-2d-gaussians-and-half-moons",
    "href": "notebooks/05-generalizing-2d.html#data-2d-gaussians-and-half-moons",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.1 Data: 2D Gaussians and Half Moons",
    "text": "6.1 Data: 2D Gaussians and Half Moons\nIn this anchoring example, we will explore how to train a score model on both the half-moons dataset and a simple 2D Gaussian. For ease of presentation, the code (as executed here) will only use the half-moons dataset but one flag at the top of the cell below, MOONS = True, can be switched to MOONS = False to switch to the 2D Gaussian dataset.\n\n\nCode\nimport jax.numpy as np \nfrom jax import random \nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import make_moons, make_circles\nimport seaborn as sns\n\n# CHANGE THIS FLAG TO FALSE TO RUN CODE WITH 2D MIXTURE GAUSSIANS.\nDATA = \"gaussians\"\nN_DATAPOINTS = 100\n\nif DATA == \"moons\":\n    X, y = make_moons(n_samples=N_DATAPOINTS, noise=0.1, random_state=99)\n    # Scale the moons dataset to be of the same scale as the Gaussian dataset.\n    X = X * 10\n\nelif DATA == \"circles\":\n    X, y = make_circles(n_samples=N_DATAPOINTS, noise=0.01, factor=0.2, random_state=99)\n    X = X * 10\n\nelse:\n    key = random.PRNGKey(55)\n    k1, k2 = random.split(key, 2)\n\n    loc1 = np.array([0., 0.])\n    cov1 = np.array([[1., 0.], [0., 1.]])\n    x1 = random.multivariate_normal(k1, loc1, cov1, shape=(int(N_DATAPOINTS / 2),))\n\n    loc2 = np.array([10., 10.])\n    cov2 = cov1 \n    x2 = random.multivariate_normal(k2, loc2, cov2, shape=(int(N_DATAPOINTS / 2),))\n\n    X = np.concatenate([x1, x2])\n\nplt.scatter(*X.T)\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"Data Dimension 1\")\nplt.ylabel(\"Data Dimension 2\")\nsns.despine()\n\n\n\n\n\nFigure 6.1: Sample synthetic data that we will be working with."
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#add-noise-to-data",
    "href": "notebooks/05-generalizing-2d.html#add-noise-to-data",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.2 Add noise to data",
    "text": "6.2 Add noise to data\nNext we noise up the data. Strictly speaking with a constant drift term, we need only parameterize our diffusion term using t (time) and don’t really need to use diffrax’s SDE capabilities. We can noise up data by applying a draw from an isotropic Gaussian with covariance equal to the time elapsed.\n\n\nCode\nfrom jax import vmap\nfrom functools import partial\nimport seaborn as sns \n\ndef noise_batch(key, X: np.ndarray, t: float) -&gt; np.ndarray:\n    \"\"\"Noise up one batch of data.\n    \n    :param x: One batch of data.\n        Should be of shape (1, n_dims).\n    :param t: Time scale at which to noise up.\n    :returns: A NumPy array of noised up data.\n    \"\"\"\n    if t == 0.0:\n        return X\n    cov = np.eye(len(X)) * t\n    return X + random.multivariate_normal(key=key, mean=np.zeros(len(X)), cov=cov)\n\n\ndef noise(key, X, t):\n    keys = random.split(key, num=len(X))\n    return vmap(partial(noise_batch, t=t))(keys, X)\n\nfrom jax import random \n\nfig, axes = plt.subplots(figsize=(8, 8), nrows=3, ncols=3, sharex=True, sharey=True)\n\nts = np.linspace(0.001, 10, 9)\nkey = random.PRNGKey(99)\nnoise_level_keys = random.split(key, 9)\nnoised_datas = []\nfor t, ax, key in zip(ts, axes.flatten(), noise_level_keys):\n    noised_data = noise(key, X, t)\n    noised_datas.append(noised_data)\n    ax.scatter(noised_data[:, 0], noised_data[:, 1], alpha=0.1)\n    ax.set_title(f\"{t:.2f}\")\nnoised_datas = np.stack(noised_datas)\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nFigure 6.2: Synthetic data at different noise scales.\n\n\n\n\nAs a sanity-check, we should ensure that noised_data’s shape is (time, batch, n_data_dims):\n\nnoised_datas.shape\n\n(9, 100, 2)\n\n\nIndeed it is!"
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#score-model-definition",
    "href": "notebooks/05-generalizing-2d.html#score-model-definition",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.3 Score model definition",
    "text": "6.3 Score model definition\nNow, we can set up a score model to be trained on each time point’s noised-up data. Here, we are going to use a feed forward neural network. The neural network needs to accept x and t; as with the previous chapter, we will be using a single neural network that learns to map input data and time to the approximated score function.\n\nfrom score_models.models.sde import SDEScoreModel\n\nIf you are curious, you can see how the SDEScoreModel class is defined below.\n\nfrom inspect import getsource\n\nprint(getsource(SDEScoreModel))\n\nclass SDEScoreModel(eqx.Module):\n    \"\"\"Time-dependent score model.\n\n    We choose an MLP here with 2 inputs (`x` and `t` concatenated),\n    and output a scalar which is the estimated score.\n    \"\"\"\n\n    mlp: eqx.Module\n\n    def __init__(\n        self,\n        data_dims=2,\n        width_size=256,\n        depth=2,\n        activation=nn.softplus,\n        key=random.PRNGKey(45),\n    ):\n        \"\"\"Initialize module.\n\n        :param data_dims: The number of data dimensions.\n            For example, 2D Gaussian data would have data_dims = 2.\n        :param width_size: Width of the hidden layers.\n        :param depth: Number of hidden layers.\n        :param activation: Activation function.\n            Should be passed in uncalled.\n        :param key: jax Random key value pairs.\n        \"\"\"\n        self.mlp = eqx.nn.MLP(\n            in_size=data_dims + 1,  # +1 for the time dimension\n            out_size=data_dims,\n            width_size=width_size,\n            depth=depth,\n            activation=activation,\n            key=key,\n        )\n\n    @eqx.filter_jit\n    def __call__(self, x: np.ndarray, t: float):\n        \"\"\"Forward pass.\n\n        :param x: Data. Should be of shape (1, :),\n            as the model is intended to be vmapped over batches of data.\n        :param t: Time in the SDE.\n        :returns: Estimated score of a Gaussian.\n        \"\"\"\n        t = np.array([t])\n        x = np.concatenate([x, t])\n        return self.mlp(x)\n\n\n\nThe key design choice here is that time t is made part of the MLP’s input by concatenation with x.\nAs always, we need a sanity-check that the model’s forward pass works:\n\nfrom functools import partial\nfrom jax import vmap \n\nmodel = SDEScoreModel(data_dims=2, depth=3)\nt = 3.0\n\nX_noised = noise(key, X, t)\nout = vmap(partial(model, t=t))(X_noised)\nout.shape\n\n(100, 2)\n\n\nBecause the shape is correct, we can be confident in the forward pass of the model working correctly."
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#loss-function",
    "href": "notebooks/05-generalizing-2d.html#loss-function",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.4 Loss function",
    "text": "6.4 Loss function\nNow, we need the score matching loss function; it is identical to the one we used in the previous chapter.\n\nfrom score_models.losses import joint_sde_score_matching_loss\n\nLet’s make sure that the loss function works without error first. Once again, this is a good practice sanity check to perform before we\n\nmodel = SDEScoreModel(data_dims=2)\njoint_sde_score_matching_loss(model, noised_datas, ts=ts)\n\nArray(1.3476348, dtype=float32)\n\n\nAs a sanity-check again, let us make sure that we can take the gradient of the loss function as well. To do so, we will use Equinox’s filter_value_and_grad, which is a fancy version of JAX’s value_and_grad that ensures that we calculate value_and_grad only on array-like arguments.\n\nimport equinox as eqx \n\ndloss = eqx.filter_value_and_grad(joint_sde_score_matching_loss)\nvalue, grads = dloss(model, noised_datas, ts=ts)\nvalue\n\nArray(1.3476348, dtype=float32)"
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#train-model",
    "href": "notebooks/05-generalizing-2d.html#train-model",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.5 Train model",
    "text": "6.5 Train model\nNow that we’ve seen the gradient function perform without errors, let’s train the model.\n\nimport optax\nfrom tqdm.auto import tqdm\nfrom jax import nn, jit\nfrom score_models.losses import l2_norm\nfrom typing import Union, Callable\n\nmodel = SDEScoreModel(depth=2, activation=nn.relu)\n\ndef lossfunc(\n    model: Union[eqx.Module, Callable], noised_data_all: np.ndarray, ts: np.ndarray\n):\n    loss = joint_sde_score_matching_loss(model, noised_data_all, ts)\n    loss += l2_norm(model, noised_data_all) * 0.01\n    return loss \n\noptimizer = optax.chain(\n    optax.adam(5e-2),\n    # optax.clip(1e-5),\n)\n\nopt_state = optimizer.init(eqx.filter(model, eqx.is_array))\ndloss = eqx.filter_value_and_grad(lossfunc)\n\n@jit\ndef training_step(model, opt_state, noised_datas, ts):\n    loss_score, grads = dloss(model, noised_datas, ts)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    return model, opt_state, loss_score\n\n\n\nn_steps = 20_000\niterator = tqdm(range(n_steps))\nloss_history = []\nkey = random.PRNGKey(555)\nkeys = random.split(key, n_steps)\n\nupdated_score_model = model\nfor step in iterator:\n    loss_score, grads = dloss(updated_score_model, noised_datas, ts)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    updated_score_model = eqx.apply_updates(updated_score_model, updates)\n    # updated_score_model, opt_state, loss_score = training_step(updated_score_model, opt_state, noised_datas, ts)\n    iterator.set_description(f\"Score· {loss_score:.2f}\")\n    loss_history.append(float(loss_score))\n\n\n\n\nLet’s plot the losses so we can have visual confirmation that we have trained the model to convergence.\n\n\nCode\nplt.plot(loss_history)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score Matching Loss\")\nplt.title(\"Score Matching Loss History\")\nsns.despine()\n\n\n{#fig-training-loss· width=589 height=449}"
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#visualize-gradient-field",
    "href": "notebooks/05-generalizing-2d.html#visualize-gradient-field",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.6 Visualize gradient field",
    "text": "6.6 Visualize gradient field\nIn this particular case, because we have 2D data, one way of confirming that we have trained the model correctly is to look at the gradient field given by our trained score model. We will compare a trained model (on the left) to an untrained model (on the right). We should see that the gradient field points to the direction of highest data density,\n\n\nCode\nbasic_size = 5\n\nfig, axes = plt.subplots(\n    figsize=(2 * basic_size, len(noised_datas) * basic_size),\n    nrows=len(noised_datas), \n    ncols=2, \n    sharex=True, \n    sharey=True\n)\ninit_model = SDEScoreModel()\n\n\nfor idx in range(len(noised_datas)):\n\n    ax_row = axes[idx, :]\n    n_points = 20\n    xs = np.linspace(noised_datas[-1][:, 0].min(), noised_datas[-1][:, 0].max(), n_points)\n    ys = np.linspace(noised_datas[-1][:, 1].min(), noised_datas[-1][:, 1].max(), n_points)\n    xxs, yys = np.meshgrid(xs, ys)\n\n    x_y_pair = np.vstack([xxs.flatten(), yys.flatten()]).T\n    x_y_pair.shape\n\n    gradient_field = vmap(partial(updated_score_model, t=ts[idx]))(x_y_pair)\n\n    vect_length_scale = 1\n    vect_width = 0.1\n\n    for xy_pair, vect in zip(x_y_pair, gradient_field):\n        ax_row[0].arrow(*xy_pair, *vect * vect_length_scale, width=vect_width, alpha=0.1)    \n    ax_row[0].scatter(*noised_datas[idx].T, alpha=0.1, color=\"black\")\n    ax_row[0].set_xlim(noised_datas[idx][:, 0].min() - 1, noised_datas[idx][:, 0].max() + 1)\n    ax_row[0].set_ylim(noised_datas[idx][:, 1].min() - 1, noised_datas[idx][:, 1].max() + 1)\n    ax_row[0].set_title(f\"Trained Score Model at t={ts[idx]:.2f}\")\n    ax_row[0].set_xlabel(\"Data Dim 1\")\n    ax_row[0].set_ylabel(\"Data Dim 2\")\n\n\n    gradient_field = vmap(partial(init_model, t=ts[idx]))(x_y_pair)\n\n    for xy_pair, vect in zip(x_y_pair, gradient_field):\n        ax_row[1].arrow(*xy_pair, *vect * vect_length_scale, width=vect_width, alpha=0.1)    \n    ax_row[1].scatter(*noised_datas[idx].T, alpha=0.1, color=\"black\")\n    ax_row[1].set_xlim(noised_datas[idx][:, 0].min() - 1, noised_datas[idx][:, 0].max() + 1)\n    ax_row[1].set_ylim(noised_datas[idx][:, 1].min() - 1, noised_datas[idx][:, 1].max() + 1)\n    ax_row[1].set_title(f\"Untrained Score Model at t={ts[idx]:.2f}\")\n    ax_row[1].set_xlabel(\"Data Dim 1\")\n    ax_row[1].set_ylabel(\"Data Dim 2\")\n\n    sns.despine()\n\n\n\n\n\nFigure 6.3: Gradient field of trained vs. untrained models at varying time points (corresponding to different noise scales).\n\n\n\n\nNotice how the gradient field on the right half of Figure 6.3 consistently ignores the density of data, whereas the gradient field on the left half of Figure 6.3 consistently points towards areas of high density."
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#probability-flow-ode",
    "href": "notebooks/05-generalizing-2d.html#probability-flow-ode",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "6.7 Probability Flow ODE",
    "text": "6.7 Probability Flow ODE\nWith the gradient fields confirmed to be correct, we can set up the probability flow ODE.\nWe need a constant drift term, a time-dependent diffusion term, and finally, the updated score model inside there.\n\ndef constant_drift(t, y, args):\n    \"\"\"Constant drift term.\"\"\"\n    return 0\n\ndef time_dependent_diffusion(t, y, args):\n    \"\"\"Diffusion term that increases with time.\"\"\"\n    return t * np.eye(2)\n\n\ndef reverse_drift(t: float, y: float, args: tuple):\n    f = constant_drift(t, y, args)  # always 0, so we can, in principle, take this term out.\n    g = time_dependent_diffusion(t, y, args)\n    s = updated_score_model(y, t)\n    # Extract out the diagonal because we assume isotropic Gaussian noise is applied.\n    return f - 0.5 * np.diagonal(np.linalg.matrix_power(g, 2)) * s\n\nfrom diffrax import ODETerm, Tsit5, SaveAt, diffeqsolve\n\nclass ODE(eqx.Module):\n    drift: callable\n\n    def __call__(self, ts: np.ndarray, y0: float):\n        term = ODETerm(self.drift)\n        solver = Tsit5()\n        saveat = SaveAt(ts=ts, dense=True)\n        sol = diffeqsolve(\n            term, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat\n        )\n        return vmap(sol.evaluate)(ts)\n\nNow, let’s plot the probability flow trajectories from a random sampling of starter points.\n\n\nCode\node = ODE(reverse_drift)\nts = np.linspace(9, 0.0001, 43)\nkey = random.PRNGKey(39)\ny0s = random.multivariate_normal(key, mean=np.array([0.0, 0.0]), cov=np.eye(2), shape=(50,)) * 30 + np.ones(2) * 5\n\ntrajectories = []\nfor y0 in y0s:\n    trajectory = ode(ts, y0)\n    trajectories.append(trajectory)\ntrajectories = np.stack(trajectories)\n\nfrom celluloid import Camera\n\nfig, axes = plt.subplots()\n\ncamera = Camera(fig)\n# Plot the noised datas as a background\nplt.scatter(*noised_datas[0].T, alpha=0.05, color=\"black\")\n\n\nfor idx in range(len(ts)):\n    plt.scatter(*noised_datas[0].T, alpha=0.05, color=\"black\")\n    plt.scatter(*trajectories[:, idx, :].T, marker=\"o\", color=\"blue\")\n    plt.gca().set_aspect(\"equal\")\n    plt.xlabel(\"Data Dim 1\")\n    plt.ylabel(\"Data Dim 2\")\n    plt.xlim(-20, 20)\n    plt.ylim(-20, 20)\n    sns.despine()\n    camera.snap()\n\nanimation = camera.animate()\nanimation.save(\"probability-flow-ode.mp4\", writer=\"ffmpeg\", dpi=300);\n\n\n\nfrom IPython.display import HTML\n\nHTML(animation.to_jshtml())\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\nFigure 6.4: Probability flow ODE and trajectories from a variety of randomly-chosen starting points. Circles mark the starting location, while diamonds mark the ending location of each trajectory.\n\n\n\nAs we can see in Figure 6.4, with an (admittedly not so) random selection of starter points, we can run the probability flow ODE in reverse time to get data coordinates that are distributed like our original starter data… without knowing the original data generating distribution! This is the whole spirit of score-based models, and in this chapter, we explored how to make that happen in a non-trivial 2D case. In principle, we could run with any kind of numerical data, such as images (where the original application of score models was done), or numerically embedded text (or protein sequences) from an encoder-decoder pair’s encoder module."
  },
  {
    "objectID": "notebooks/05-generalizing-2d.html#footnotes",
    "href": "notebooks/05-generalizing-2d.html#footnotes",
    "title": "6  Generalizing to Higher Dimensions",
    "section": "",
    "text": "Of course, yes – this is a rhetorical question – and the more important point here is figuring out what we need to do to generalize beyond 1D.↩︎"
  }
]